{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlywzJykjt7_"
      },
      "source": [
        "We now have our ready encoder architecture, now we need to make the decoder which takes input of encoders and then preidcting out the predictions for our reactions which will predict the reagent, solvent and catalyst in this case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Torvu19BkYjx"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yrm9rKotjTfd",
        "outputId": "995a34d1-c089-4be8-a4ef-f298d7c44a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.10/dist-packages (2022.9.5)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (9.4.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.11.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-geometric rdkit-pypi fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eBVbQeI2keuS"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import spacy\n",
        "import fasttext\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,dataloader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence,pad_packed_sequence,pad_sequence\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "day2MEbb7vH9"
      },
      "outputs": [],
      "source": [
        "from rdkit import RDLogger\n",
        "\n",
        "Chem.MolFromSmiles('c1cncc1')\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "Chem.MolFromSmiles('c1cncc1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LODSFVELoLPV"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JTvgn8OAWSv",
        "outputId": "dbc1b815-52e9-47cc-8c98-562c1a3c9a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCqR7162_0ZG"
      },
      "source": [
        "# Importing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "MeLSwHSD_15_",
        "outputId": "296a49d6-d4af-4917-c7c7-eb0ae451fa20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         canonic_rxn  \\\n",
              "0  CCO.O=S1(=O)C=Cc2ccccc21.[H][H].[Pd]>>O=S1(=O)...   \n",
              "1  O=S1(=O)C=Cc2ccccc21.[Na+].[OH-].[Zn]>>O=S1(=O...   \n",
              "2  CCO.O=S1(=O)C=Cc2ccccc21.[Pd]>>O=S1(=O)CCc2ccc...   \n",
              "3  CO.O=C1CCCN1C1CCN(Cc2ccccc2)CC1.O=C[O-].[NH4+]...   \n",
              "4  CC(C)(C)OC(=O)N1CC2CC(CN(Cc3ccccc3)C2)C1.CCO.[...   \n",
              "\n",
              "                                       rxnmapper_aam  \\\n",
              "0  CCO.[O:1]=[S:2]1(=[O:3])[CH:4]=[CH:5][c:6]2[cH...   \n",
              "1  [O:1]=[S:2]1(=[O:3])[CH:4]=[CH:5][c:6]2[cH:7][...   \n",
              "2  CCO.[O:1]=[S:2]1(=[O:3])[CH:4]=[CH:5][c:6]2[cH...   \n",
              "3  CO.[O:1]=[C:2]1[CH2:3][CH2:4][CH2:5][N:6]1[CH:...   \n",
              "4  [CH3:1][C:2]([CH3:3])([CH3:4])[O:5][C:6](=[O:7...   \n",
              "\n",
              "                                             reagent   solvent  \\\n",
              "0                                           hydrogen   ethanol   \n",
              "1                              sodium hydroxide|zinc     empty   \n",
              "2            palladium on activated charcoal|ethanol     empty   \n",
              "3  palladium 10 on activated carbon|ammonium formate  methanol   \n",
              "4                                           hydrogen   ethanol   \n",
              "\n",
              "                          catalyst  yield  \n",
              "0  palladium on activated charcoal  100.0  \n",
              "1                            empty    0.0  \n",
              "2                            empty    0.0  \n",
              "3                            empty  100.0  \n",
              "4  palladium on activated charcoal   51.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-429583e8-3070-4e9e-b1e7-3e2e7a2a0bc4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>canonic_rxn</th>\n",
              "      <th>rxnmapper_aam</th>\n",
              "      <th>reagent</th>\n",
              "      <th>solvent</th>\n",
              "      <th>catalyst</th>\n",
              "      <th>yield</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CCO.O=S1(=O)C=Cc2ccccc21.[H][H].[Pd]&gt;&gt;O=S1(=O)...</td>\n",
              "      <td>CCO.[O:1]=[S:2]1(=[O:3])[CH:4]=[CH:5][c:6]2[cH...</td>\n",
              "      <td>hydrogen</td>\n",
              "      <td>ethanol</td>\n",
              "      <td>palladium on activated charcoal</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>O=S1(=O)C=Cc2ccccc21.[Na+].[OH-].[Zn]&gt;&gt;O=S1(=O...</td>\n",
              "      <td>[O:1]=[S:2]1(=[O:3])[CH:4]=[CH:5][c:6]2[cH:7][...</td>\n",
              "      <td>sodium hydroxide|zinc</td>\n",
              "      <td>empty</td>\n",
              "      <td>empty</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CCO.O=S1(=O)C=Cc2ccccc21.[Pd]&gt;&gt;O=S1(=O)CCc2ccc...</td>\n",
              "      <td>CCO.[O:1]=[S:2]1(=[O:3])[CH:4]=[CH:5][c:6]2[cH...</td>\n",
              "      <td>palladium on activated charcoal|ethanol</td>\n",
              "      <td>empty</td>\n",
              "      <td>empty</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CO.O=C1CCCN1C1CCN(Cc2ccccc2)CC1.O=C[O-].[NH4+]...</td>\n",
              "      <td>CO.[O:1]=[C:2]1[CH2:3][CH2:4][CH2:5][N:6]1[CH:...</td>\n",
              "      <td>palladium 10 on activated carbon|ammonium formate</td>\n",
              "      <td>methanol</td>\n",
              "      <td>empty</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CC(C)(C)OC(=O)N1CC2CC(CN(Cc3ccccc3)C2)C1.CCO.[...</td>\n",
              "      <td>[CH3:1][C:2]([CH3:3])([CH3:4])[O:5][C:6](=[O:7...</td>\n",
              "      <td>hydrogen</td>\n",
              "      <td>ethanol</td>\n",
              "      <td>palladium on activated charcoal</td>\n",
              "      <td>51.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-429583e8-3070-4e9e-b1e7-3e2e7a2a0bc4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-429583e8-3070-4e9e-b1e7-3e2e7a2a0bc4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-429583e8-3070-4e9e-b1e7-3e2e7a2a0bc4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-16fb86b9-576d-4093-a6fe-805ccb3713ca\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-16fb86b9-576d-4093-a6fe-805ccb3713ca')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-16fb86b9-576d-4093-a6fe-805ccb3713ca button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Chiros Dataset/choriso_public.tsv',sep='\\t')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cZdLhbVUAmc2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NODS63QhkjDI"
      },
      "source": [
        "# Features File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7iET2R-Rkhho"
      },
      "outputs": [],
      "source": [
        "def featurize_molecule(mol):\n",
        "    # Compute Morgan fingerprints for each atom\n",
        "    atom_features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        idx = atom.GetIdx()\n",
        "        atom_feature = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, atomIndices=[idx])\n",
        "        atom_features.append(np.array(atom_feature))\n",
        "\n",
        "    return np.array(atom_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XI34cufKkmP0"
      },
      "outputs": [],
      "source": [
        "def smiles_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    # Add explicit hydrogens\n",
        "    mol = Chem.AddHs(mol)\n",
        "\n",
        "    # Generate 3D coordinates for visualization\n",
        "    AllChem.EmbedMolecule(mol, randomSeed=42)  # You can choose any seed value\n",
        "\n",
        "    # Get atom features and adjacency matrix\n",
        "    num_atoms = mol.GetNumAtoms()\n",
        "    atom_features = np.zeros((num_atoms, 3))  # You may need to adjust the feature dimensions\n",
        "    adjacency_matrix = np.zeros((num_atoms, num_atoms))\n",
        "\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        adjacency_matrix[i, j] = adjacency_matrix[j, i] = 1  # Adjacency matrix is symmetric\n",
        "\n",
        "    for atom in mol.GetAtoms():\n",
        "        idx = atom.GetIdx()\n",
        "        atom_features[idx, 0] = atom.GetAtomicNum()  # Atom type or atomic number\n",
        "        atom_features[idx, 1] = atom.GetTotalNumHs()  # Number of hydrogen atoms\n",
        "        atom_features[idx, 2] = atom.GetFormalCharge()  # Formal charge\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    atom_features = torch.tensor(atom_features, dtype=torch.float)\n",
        "\n",
        "    # Create edge_index using the adjacency matrix\n",
        "    edge_index = torch.tensor(np.column_stack(np.where(adjacency_matrix)), dtype=torch.long)\n",
        "\n",
        "    # Create PyTorch Geometric data object\n",
        "    data = Data(x=atom_features, edge_index=edge_index.t().contiguous())  # Transpose edge_index\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpOWk0E4kpYW"
      },
      "source": [
        "# GCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "4YgoQX9wkoBE"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, input_size, max_len=1000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = nn.Embedding(max_len, input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        positions = torch.arange(0, x.size(1), device=device).unsqueeze(0)\n",
        "        positions = positions.expand(x.size(0), -1)  # Expand along the batch dimension\n",
        "        positions = positions.to(device)\n",
        "        return x + self.encoding(positions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "khkEcrJzktcj"
      },
      "outputs": [],
      "source": [
        "class DistanceAttentionEncoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(DistanceAttentionEncoder, self).__init__()\n",
        "\n",
        "        self.embedding = PositionalEncoding(input_size)\n",
        "        self.encoder = nn.Linear(input_size, hidden_size)\n",
        "        self.decoder = nn.Linear(hidden_size, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def pairwise_distances(self, x):\n",
        "        distances = torch.norm(x[:, None, :] - x, dim=-1, p=2)\n",
        "        return distances\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        embedded_sequence = self.embedding(input_sequence)\n",
        "        encoded_sequence = self.encoder(embedded_sequence)\n",
        "        attention_scores = self.decoder(torch.tanh(encoded_sequence))\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "        context_vector = torch.sum(encoded_sequence * attention_weights, dim=1)\n",
        "\n",
        "        return context_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "UBEyoidGk25_"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, attention_heads=1):\n",
        "        super().__init__()\n",
        "        self.W_q = nn.Linear(in_channels, out_channels)\n",
        "        self.W_k = nn.Linear(in_channels, out_channels)\n",
        "        self.W_v = nn.Linear(in_channels, out_channels)\n",
        "        self.attention_heads = attention_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.W_q(x)\n",
        "        k = self.W_k(x)\n",
        "        v = self.W_v(x)\n",
        "\n",
        "        q = q.view(-1, self.attention_heads, q.size(-1))\n",
        "        k = k.view(-1, self.attention_heads, k.size(-1))\n",
        "        v = v.view(-1, self.attention_heads, v.size(-1))\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(q.size(-1), dtype=torch.float32))\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, v).view(x.size(0), -1)\n",
        "\n",
        "        return attention_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "DTKWOBFRkvIo"
      },
      "outputs": [],
      "source": [
        "class GATModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1, external_attention_heads=None):\n",
        "        super(GATModel, self).__init__()\n",
        "\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
        "        self.external_attention = MultiHeadAttention(hidden_channels * heads, hidden_channels, attention_heads=external_attention_heads)\n",
        "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)\n",
        "        self.external_attention_heads = external_attention_heads\n",
        "\n",
        "    def forward(self, data):\n",
        "        data = data.to(device)\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        if self.external_attention_heads is not None:\n",
        "            external_attention_output = self.external_attention(x)\n",
        "            x = torch.cat([x, external_attention_output], dim=-1)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        self.distance_attention_encoder = DistanceAttentionEncoder(x.size(1), hidden_size=64).to(device)\n",
        "        distance_attention_output = self.distance_attention_encoder(x.unsqueeze(0))\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmNO3m8Nk-Il"
      },
      "source": [
        "# Seperating compunds in the SMILES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkkdPKVjkxgN",
        "outputId": "1a20e4e7-12fd-4a94-c03c-32b3820b2483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reactant: O=C1CCCN1C1CCN(Cc2ccccc2)CC1\n",
            "Product: O=C1CCCN1C1CCNCC1\n"
          ]
        }
      ],
      "source": [
        "def separate_compounds(smiles_reaction):\n",
        "    # Split the reaction string using '>>' as the separator\n",
        "    compounds = smiles_reaction.split(\">>\")\n",
        "\n",
        "    # Ensure that there are exactly two compounds\n",
        "    if len(compounds) == 2:\n",
        "        reactant = compounds[0].strip()\n",
        "        product = compounds[1].strip()\n",
        "        return reactant, product\n",
        "    else:\n",
        "        raise ValueError(\"Invalid SMILES reaction format. Expected one '>>' separator.\")\n",
        "\n",
        "# Given SMILES reaction\n",
        "smiles_reaction = \"O=C1CCCN1C1CCN(Cc2ccccc2)CC1>>O=C1CCCN1C1CCNCC1\"\n",
        "\n",
        "# Separate compounds\n",
        "reactant, product = separate_compounds(smiles_reaction)\n",
        "\n",
        "# Print the separated compounds\n",
        "print(\"Reactant:\", reactant)\n",
        "print(\"Product:\", product)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3hlaPvzlqzb"
      },
      "source": [
        "# Cross Attention for both\n",
        "Using cross attention to concatenate for both the compounds into a single embedding space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "z51KzMZ_lm_0"
      },
      "outputs": [],
      "source": [
        "def concatenate_with_cross_attention(emb1, emb2, out_channels, heads):\n",
        "    if emb1.shape[0] > emb2.shape[0]:\n",
        "        linear_projection = nn.Linear(emb2.shape[0], emb1.shape[0]).to(device)\n",
        "        emb2 = linear_projection(emb2.T).T\n",
        "    elif emb1.shape[0] < emb2.shape[0]:\n",
        "        linear_projection = nn.Linear(emb1.shape[0], emb2.shape[0]).to(device)\n",
        "        emb1 = linear_projection(emb1.T).T\n",
        "\n",
        "    concatenated_emb = torch.cat((emb1, emb2), dim=1)\n",
        "\n",
        "    multihead_attention = nn.MultiheadAttention(embed_dim=2 * out_channels, num_heads=heads).to(device)\n",
        "    cross_attended_emb, _ = multihead_attention(concatenated_emb, concatenated_emb, concatenated_emb)\n",
        "\n",
        "    return cross_attended_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "dL2cMcK7nrFh"
      },
      "outputs": [],
      "source": [
        "def get_cross_attention_output(smiles_string, train_mode=True):\n",
        "    reactant, product = separate_compounds(smiles_string)\n",
        "    graph_data_reactant = smiles_to_graph(reactant)\n",
        "    graph_data_product = smiles_to_graph(product)\n",
        "    graph_data_reactant = graph_data_reactant.to(device)\n",
        "    graph_data_product = graph_data_product.to(device)\n",
        "\n",
        "    in_channels = graph_data_reactant.x.size(1)\n",
        "    hidden_channels = 64\n",
        "    out_channels = 32\n",
        "    heads = 2\n",
        "    gat_model_reactant = GATModel(in_channels, hidden_channels, out_channels, heads).to(device)\n",
        "\n",
        "    if train_mode:\n",
        "        gat_model_reactant.train()\n",
        "    else:\n",
        "        gat_model_reactant.eval()\n",
        "\n",
        "    output_reactant = gat_model_reactant(graph_data_reactant)\n",
        "\n",
        "    in_channels = graph_data_product.x.size(1)\n",
        "    hidden_channels = 64\n",
        "    out_channels = 32\n",
        "    heads = 2\n",
        "    gat_model_product = GATModel(in_channels, hidden_channels, out_channels, heads).to(device)\n",
        "\n",
        "    if train_mode:\n",
        "        gat_model_product.train()\n",
        "    else:\n",
        "        gat_model_product.eval()\n",
        "\n",
        "    output_product = gat_model_product(graph_data_product)\n",
        "\n",
        "    out = concatenate_with_cross_attention(output_reactant, output_product, out_channels=32, heads=2)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "rk8qs9F5nMgG"
      },
      "outputs": [],
      "source": [
        "smiles_string = \"CC(C)(C)OC(=O)N1CC2CC(CN(Cc3ccccc3)C2)C1>>CC(C)(C)OC(=O)N1CC2CNCC(C2)C1\"\n",
        "out=get_cross_attention_output(smiles_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJzQOVO5-KH4"
      },
      "source": [
        "# Vocab Size and Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "mTiNvq7aEVrO"
      },
      "outputs": [],
      "source": [
        "def pad_sequence(sequence, max_length, padding_token='<PAD>'):\n",
        "    return sequence + [padding_token] * (max_length - len(sequence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "brUmdMmURaP_"
      },
      "outputs": [],
      "source": [
        "df=df.iloc[:10000,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDLtVusHWJ-m",
        "outputId": "306af291-a791-4a2b-a9f2-6c23c89e5b11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 6 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   canonic_rxn    100 non-null    object \n",
            " 1   rxnmapper_aam  100 non-null    object \n",
            " 2   reagent        100 non-null    object \n",
            " 3   solvent        100 non-null    object \n",
            " 4   catalyst       100 non-null    object \n",
            " 5   yield          100 non-null    float64\n",
            "dtypes: float64(1), object(5)\n",
            "memory usage: 4.8+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVdngVQf-2iZ",
        "outputId": "ec3819f6-90ba-453f-d32f-c022525b3ead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reagent Vocabulary Size: 149\n",
            "Padded Reagent:\n",
            "0     [hydrogen, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, ...\n",
            "1     [sodium, hydroxide|zinc, <PAD>, <PAD>, <PAD>, ...\n",
            "2     [palladium, on, activated, charcoal|ethanol, <...\n",
            "3     [palladium, 10, on, activated, carbon|ammonium...\n",
            "4     [hydrogen, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, ...\n",
            "                            ...                        \n",
            "95    [tetrachloromethane|bromine, <PAD>, <PAD>, <PA...\n",
            "96    [thionyl, chloride, <PAD>, <PAD>, <PAD>, <PAD>...\n",
            "97    [thionyl, chloride, <PAD>, <PAD>, <PAD>, <PAD>...\n",
            "98    [oxalyl, dichloride|n, ,, n-dimethyl-formamide...\n",
            "99    [thionyl, chloride, <PAD>, <PAD>, <PAD>, <PAD>...\n",
            "Name: reagent, Length: 100, dtype: object\n",
            "\n",
            "Solvent Vocabulary Size: 34\n",
            "Padded Solvent:\n",
            "0     [ethanol, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <...\n",
            "1     [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "2     [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "3     [methanol, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, ...\n",
            "4     [ethanol, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <...\n",
            "                            ...                        \n",
            "95    [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "96    [benzene, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <...\n",
            "97    [tetrahydrofuran, <PAD>, <PAD>, <PAD>, <PAD>, ...\n",
            "98    [dichloromethane, <PAD>, <PAD>, <PAD>, <PAD>, ...\n",
            "99    [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "Name: solvent, Length: 100, dtype: object\n",
            "\n",
            "Catalyst Vocabulary Size: 39\n",
            "Padded Catalyst:\n",
            "0     [palladium, on, activated, charcoal, <PAD>, <P...\n",
            "1     [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "2     [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "3     [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "4     [palladium, on, activated, charcoal, <PAD>, <P...\n",
            "                            ...                        \n",
            "95    [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "96    [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "97    [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "98    [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "99    [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "Name: catalyst, Length: 100, dtype: object\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-a37ee099bce5>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column] = df[column].apply(lambda x: pad_sequence(word_tokenize(str(x)), max_seq_length))\n"
          ]
        }
      ],
      "source": [
        "columns_to_process = ['reagent', 'solvent', 'catalyst']\n",
        "\n",
        "for column in columns_to_process:\n",
        "    # Tokenize and build vocabularies for each column\n",
        "    tokens = [token for item in df[column] for token in word_tokenize(str(item))]\n",
        "    vocab = set(tokens)\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # Pad sequences to a common size\n",
        "    max_seq_length = max(len(token) for token in tokens)\n",
        "\n",
        "    # Pad sequences in the DataFrame\n",
        "    df[column] = df[column].apply(lambda x: pad_sequence(word_tokenize(str(x)), max_seq_length))\n",
        "\n",
        "    # Print results\n",
        "    print(f\"{column.capitalize()} Vocabulary Size: {vocab_size}\")\n",
        "    print(f\"Padded {column.capitalize()}:\\n{df[column]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILFveO43LHG2"
      },
      "source": [
        "# A full Seq2Seq Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "rknul2-9LK-Z"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size_condition1, vocab_size_condition2, vocab_size_condition3, d_model=512, nhead=8, num_layers=6, enc_dim=64, max_seq=100):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "\n",
        "        self.vocab_size_condition1 = vocab_size_condition1\n",
        "        self.vocab_size_condition2 = vocab_size_condition2\n",
        "        self.vocab_size_condition3 = vocab_size_condition3\n",
        "        self.d_model = d_model\n",
        "        self.linear_layer = nn.Linear(vocab_size_condition1, d_model)\n",
        "\n",
        "        self.embedding1 = nn.Embedding(vocab_size_condition1, d_model)\n",
        "        self.embedding2 = nn.Embedding(vocab_size_condition2, d_model)\n",
        "        self.embedding3 = nn.Embedding(vocab_size_condition3, d_model)\n",
        "\n",
        "        self.transformer_layers = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model, nhead),\n",
        "            num_layers\n",
        "        )\n",
        "\n",
        "        self.fc_condition1 = nn.Linear(d_model, vocab_size_condition1)\n",
        "        self.fc_condition2 = nn.Linear(d_model, vocab_size_condition2)\n",
        "        self.fc_condition3 = nn.Linear(d_model, vocab_size_condition3)\n",
        "\n",
        "        self.softmax_condition1 = nn.Softmax(dim=1)\n",
        "        self.softmax_condition2 = nn.Softmax(dim=1)\n",
        "        self.softmax_condition3 = nn.Softmax(dim=1)\n",
        "\n",
        "        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead)\n",
        "\n",
        "        self.linear_proj = nn.Linear(max_seq, enc_dim)\n",
        "        self.max_seq = max_seq\n",
        "\n",
        "        self.final_linear_layer1 = nn.Linear(d_model, max_seq)\n",
        "        self.final_linear_layer2 = nn.Linear(d_model, max_seq)\n",
        "        self.final_linear_layer3 = nn.Linear(d_model, max_seq)\n",
        "\n",
        "    def forward(self, encoded_input, target_sequence):\n",
        "        self.emb_cond1 = self.embedding1(target_sequence[0].to(device))\n",
        "        self.emb_cond2 = self.embedding2(target_sequence[1].to(device))\n",
        "        self.emb_cond3 = self.embedding3(target_sequence[2].to(device))\n",
        "\n",
        "        self.comb_emb = self.emb_cond1 + self.emb_cond2 + self.emb_cond3\n",
        "        self.comb_emb = self.comb_emb.squeeze(0)\n",
        "\n",
        "        memory = torch.rand(1, self.max_seq, 512).to(device)\n",
        "        trans_decoder = self.transformer_layers(self.comb_emb, memory=memory)\n",
        "        new_size = (trans_decoder.size(0) * trans_decoder.size(1), -1)\n",
        "        trans_decoder_2d = trans_decoder.view(*new_size)\n",
        "\n",
        "        proj_trans_decoder = self.linear_proj(trans_decoder_2d.T).T\n",
        "\n",
        "        tensor1 = proj_trans_decoder.unsqueeze(1)\n",
        "        tensor1 = tensor1.permute(1, 2, 0)\n",
        "        tensor2 = encoded_input.unsqueeze(0)\n",
        "\n",
        "        attention_weights = F.softmax(torch.bmm(tensor1, tensor2.permute(0, 2, 1)), dim=-1)\n",
        "        cross_attention_result = torch.bmm(attention_weights, tensor2)\n",
        "        cross_attention_result = cross_attention_result.squeeze(1)\n",
        "        new_size = (cross_attention_result.size(0) * cross_attention_result.size(1), -1)\n",
        "        cross_attention_result = cross_attention_result.view(*new_size)\n",
        "\n",
        "        final_output1 = self.final_linear_layer1(cross_attention_result.T)\n",
        "        final_output_summed1 = final_output1.sum(dim=0)\n",
        "\n",
        "        final_output2 = self.final_linear_layer2(cross_attention_result.T)\n",
        "        final_output_summed2 = final_output2.sum(dim=0)\n",
        "\n",
        "        final_output3 = self.final_linear_layer3(cross_attention_result.T)\n",
        "        final_output_summed3 = final_output3.sum(dim=0)\n",
        "\n",
        "        return F.relu(final_output_summed1), F.relu(final_output_summed2), F.relu(final_output_summed3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "gFJ51FBmwJH0"
      },
      "outputs": [],
      "source": [
        "class TransformerSeq2Seq(nn.Module):\n",
        "    def __init__(self, vocab_size_condition1, vocab_size_condition2, vocab_size_condition3, decoder_d_model=512, decoder_nhead=8, decoder_layers=6, enc_dim=64):\n",
        "        super(TransformerSeq2Seq, self).__init__()\n",
        "\n",
        "        self.decoder = TransformerDecoder(vocab_size_condition1, vocab_size_condition2, vocab_size_condition3,\n",
        "                                         d_model=decoder_d_model, nhead=decoder_nhead, num_layers=decoder_layers, enc_dim=enc_dim)\n",
        "\n",
        "    def forward(self, input_sequence, targets, train_mode=True):\n",
        "        self.hidden_state = get_cross_attention_output((input_sequence), train_mode)\n",
        "\n",
        "        self.decoder = self.decoder.to(device)\n",
        "\n",
        "        if train_mode:\n",
        "            self.decoder.train()\n",
        "        else:\n",
        "            self.decoder.eval()\n",
        "\n",
        "        output_probs_condition1, output_probs_condition2, output_probs_condition3 = self.decoder(self.hidden_state, targets)\n",
        "\n",
        "        return output_probs_condition1,output_probs_condition2,output_probs_condition3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR9aK6X0fuPv"
      },
      "source": [
        "# Conversion Into Embeddings\n",
        "As we knwo have our tokeinzed data, we need to first convert them into their respective embeddings, to show the working for we have used the token to int mapping which randomly assigns them integers and add +1 to each of the new word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "sPLsEcI2bzla"
      },
      "outputs": [],
      "source": [
        "# Dictionary to store mappings of tokens to integers\n",
        "token_to_int_mapping = {}\n",
        "\n",
        "def token_to_int(token):\n",
        "    global token_to_int_mapping\n",
        "\n",
        "    if token not in token_to_int_mapping:\n",
        "        # Assign a random number to the new token\n",
        "        random_int = len(token_to_int_mapping) + 1  # Start from 1 to avoid conflict with 0 for padding\n",
        "        token_to_int_mapping[token] = random_int\n",
        "\n",
        "    return token_to_int_mapping[token]\n",
        "\n",
        "def tokenize_and_pad_sequence(sequence, max_seq_length):\n",
        "    # Convert <PAD> values to 0\n",
        "    tokenized_sequence = [0 if token == '<PAD>' else token_to_int(token) for token in sequence]\n",
        "\n",
        "    # Pad the sequence with zeros if it's shorter than max_seq_length\n",
        "    padded_sequence = tokenized_sequence + [0] * (max_seq_length - len(tokenized_sequence))\n",
        "\n",
        "    # Convert the sequence to a PyTorch tensor\n",
        "    tensor_sequence = torch.tensor(padded_sequence, dtype=torch.long)\n",
        "\n",
        "    return tensor_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "UZMa0bYOc4GX"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 100  # Adjust this according to your requirements\n",
        "tokenized_cond1 = tokenize_and_pad_sequence(df['reagent'][1], max_seq_length)\n",
        "tensor_cond1 = tokenized_cond1.unsqueeze(0)  # Assuming batch size is 1\n",
        "\n",
        "# Example usage for the second condition (solvent)\n",
        "tokenized_cond2 = tokenize_and_pad_sequence(df['solvent'][2], max_seq_length)\n",
        "tensor_cond2 = tokenized_cond2.unsqueeze(0)  # Assuming batch size is 1\n",
        "\n",
        "# Example usage for the third condition (catalyst)\n",
        "tokenized_cond3 = tokenize_and_pad_sequence(df['catalyst'][10], max_seq_length)\n",
        "tensor_cond3 = tokenized_cond3.unsqueeze(0)  # Assuming batch size is 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM9BdB6Bcg9I",
        "outputId": "0801d439-77b3-4a48-83cd-b7b7323fa1c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "tensor_cond1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj6-vh9G-tSb"
      },
      "source": [
        "# Making DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "F4aQrGaDyAHD"
      },
      "outputs": [],
      "source": [
        "class ReactionDataset(Dataset):\n",
        "    def __init__(self, dataframe, max_seq_length):\n",
        "        self.dataframe = dataframe\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        reagent_sequence = tokenize_and_pad_sequence(df['reagent'][idx], max_seq_length)\n",
        "        tensor_reagent = torch.tensor(reagent_sequence).unsqueeze(0)\n",
        "\n",
        "        solvent_sequence = tokenize_and_pad_sequence(df['solvent'][idx], max_seq_length)\n",
        "        tensor_solvent = torch.tensor(solvent_sequence).unsqueeze(0)\n",
        "\n",
        "        catalyst_sequence = tokenize_and_pad_sequence(df['catalyst'][idx], max_seq_length)\n",
        "        tensor_catalyst = torch.tensor(catalyst_sequence).unsqueeze(0)\n",
        "\n",
        "\n",
        "        canonic_rxn = self.dataframe['canonic_rxn'][idx]\n",
        "\n",
        "        return canonic_rxn,[tensor_reagent,tensor_solvent,tensor_catalyst]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "oCCq2VE9un_l"
      },
      "outputs": [],
      "source": [
        "reaction_dataset = ReactionDataset(df, max_seq_length=100)\n",
        "reaction_dataloader = DataLoader(reaction_dataset, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LLIdECADcsF"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD-laSDtrNSr",
        "outputId": "6fa58017-7250-481f-abf0-c7d3490638ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-129-ae623d6fc907>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tensor_reagent = torch.tensor(reagent_sequence).unsqueeze(0)\n",
            "<ipython-input-129-ae623d6fc907>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tensor_solvent = torch.tensor(solvent_sequence).unsqueeze(0)\n",
            "<ipython-input-129-ae623d6fc907>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tensor_catalyst = torch.tensor(catalyst_sequence).unsqueeze(0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 301.7540\n",
            "Epoch 2/5, Loss: 296.9651\n",
            "Epoch 3/5, Loss: 293.9265\n",
            "Epoch 4/5, Loss: 316.3997\n",
            "Epoch 5/5, Loss: 295.0976\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, dataloader, optimizer, num_epochs=10):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for inputs, targets in reaction_dataloader:\n",
        "            inputs = inputs[0]\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs1, outputs2, outputs3 = model(inputs, targets)\n",
        "            outputs1 = outputs1.unsqueeze(0)\n",
        "            outputs2 = outputs2.unsqueeze(0)\n",
        "            outputs3 = outputs3.unsqueeze(0)\n",
        "\n",
        "            outputs1 = outputs1.float()\n",
        "            outputs2 = outputs2.float()\n",
        "            outputs3 = outputs3.float()\n",
        "\n",
        "            targets1 = targets[0].squeeze(0).float().to(device)\n",
        "            targets2 = targets[1].squeeze(0).float().to(device)\n",
        "            targets3 = targets[2].squeeze(0).float().to(device)\n",
        "\n",
        "            loss1 = F.mse_loss(outputs1, targets1)\n",
        "            loss2 = F.mse_loss(outputs2, targets2)\n",
        "            loss3 = F.mse_loss(outputs3, targets3)\n",
        "\n",
        "            epoch_loss = loss1 + loss2 + loss3\n",
        "\n",
        "            epoch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += epoch_loss\n",
        "\n",
        "        average_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}\")\n",
        "\n",
        "    return average_loss\n",
        "\n",
        "# Assuming your model and dataloader are defined somewhere\n",
        "# Create an instance of the model\n",
        "model = TransformerSeq2Seq(vocab_size_condition1=1000, vocab_size_condition2=1000, vocab_size_condition3=1000)\n",
        "\n",
        "# Assuming you have a DataLoader named reaction_dataloader\n",
        "# Initialize the loss function and optimizer\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "loss=train_model(model, reaction_dataloader, optimizer, num_epochs=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferencing"
      ],
      "metadata": {
        "id": "PgdNhurqDDv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_key_by_value(dictionary, value):\n",
        "    for key, val in dictionary.items():\n",
        "        if val == value:\n",
        "            return key\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "RkQW7R9gMZgK"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_input(encoded_input, token_to_int_mapping):\n",
        "    decoded_input=[]\n",
        "    for idx in encoded_input:\n",
        "      if int(idx) !=0 and find_key_by_value(token_to_int_mapping, int(idx)) is not None:\n",
        "        decoded_input.append(find_key_by_value(token_to_int_mapping, int(idx)))\n",
        "    return decoded_input\n"
      ],
      "metadata": {
        "id": "We4_nMTNJFPJ"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "N_8HCt5kggLq"
      },
      "outputs": [],
      "source": [
        "def inference(model,input_encoding):\n",
        "    # Load the trained model\n",
        "    # model = TransformerSeq2Seq(vocab_size_condition1=1000, vocab_size_condition2=1000, vocab_size_condition3=1000)\n",
        "    # model_state_dict = torch.load('your_model_checkpoint.pth')\n",
        "    # model.load_state_dict(model_state_dict)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    # Create a placeholder tensor for target_sequence during inference\n",
        "    placeholder_target = [torch.zeros((1,1,100)).long().to(device),torch.zeros((1,1,100)).long().to(device),torch.zeros((1,1,100)).long().to(device)]  # Adjust max_sequence_length accordingly7\n",
        "\n",
        "\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        # Pass only input_encoding and the placeholder_target to the model during inference\n",
        "        output_probs_condition1, output_probs_condition2, output_probs_condition3 = model(input_encoding, targets=placeholder_target, train_mode=False)\n",
        "\n",
        "    # Post-process output if necessary\n",
        "    output_condition1 = output_probs_condition1.cpu().numpy()\n",
        "    output_condition2 = output_probs_condition2.cpu().numpy()\n",
        "    output_condition3 = output_probs_condition3.cpu().numpy()\n",
        "\n",
        "    decoded_condition1 = decode_input(output_probs_condition1, token_to_int_mapping)\n",
        "    decoded_condition2 = decode_input(output_probs_condition2, token_to_int_mapping)\n",
        "    decoded_condition3 = decode_input(output_probs_condition3, token_to_int_mapping)\n",
        "\n",
        "    reactant,product= separate_compounds(input_encoding)\n",
        "\n",
        "    print(\"This is the original Reaction\", input_encoding,'The reactant is ',reactant, 'The product is ',product)\n",
        "    print(\"The Reagant of the original Reaction is\",' '.join(decoded_condition1))\n",
        "    print(\"The Solvent of the original Reaction is\",' '.join(decoded_condition2))\n",
        "    print(\"The Catalyst of the original Reaction is\",' '.join(decoded_condition3))\n",
        "\n",
        "\n",
        "    return output_condition1, output_condition2, output_condition3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "o1,o2,o3=inference(model,'CC(C)(C)OC(=O)N1CC2CC(CN(Cc3ccccc3)C2)C1>>CC(C)(C)OC(=O)N1CC2CNCC(C2)C1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv1MSX1yFUhg",
        "outputId": "7cb136ef-1f74-4b3c-b581-1cc066e7b868"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the original Reaction CC(C)(C)OC(=O)N1CC2CC(CN(Cc3ccccc3)C2)C1>>CC(C)(C)OC(=O)N1CC2CNCC(C2)C1 The reactant is  CC(C)(C)OC(=O)N1CC2CC(CN(Cc3ccccc3)C2)C1 The product is  CC(C)(C)OC(=O)N1CC2CNCC(C2)C1\n",
            "The Reagant of the original Reaction is tris sodium\n",
            "The Solvent of the original Reaction is \n",
            "The Catalyst of the original Reaction is empty empty empty empty sodium\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(o1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrNLzUXSQuq-",
        "outputId": "66761ef5-bc28-46fe-cf63-35ac57a2c5ff"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[31.234661   5.64433    6.460329   0.        15.717079   3.0150023\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.       ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OEoExCpiQvUF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}