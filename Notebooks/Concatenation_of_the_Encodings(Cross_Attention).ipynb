{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The Previous Code of GCN we wrote was for a single compound, now for the reaction we now have two compounds, encode them and make a single encoding space, for this we will be using cross attention mechanism"
      ],
      "metadata": {
        "id": "AlywzJykjt7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "Torvu19BkYjx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrm9rKotjTfd"
      },
      "outputs": [],
      "source": [
        "!pip install torch-geometric rdkit-pypi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "eBVbQeI2keuS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "LODSFVELoLPV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features File"
      ],
      "metadata": {
        "id": "NODS63QhkjDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def featurize_molecule(mol):\n",
        "    # Compute Morgan fingerprints for each atom\n",
        "    atom_features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        idx = atom.GetIdx()\n",
        "        atom_feature = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, atomIndices=[idx])\n",
        "        atom_features.append(np.array(atom_feature))\n",
        "\n",
        "    return np.array(atom_features)"
      ],
      "metadata": {
        "id": "7iET2R-Rkhho"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smiles_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    # Add explicit hydrogens\n",
        "    mol = Chem.AddHs(mol)\n",
        "\n",
        "    # Generate 3D coordinates for visualization\n",
        "    AllChem.EmbedMolecule(mol, randomSeed=42)  # You can choose any seed value\n",
        "\n",
        "    # Get atom features and adjacency matrix\n",
        "    num_atoms = mol.GetNumAtoms()\n",
        "    atom_features = np.zeros((num_atoms, 3))  # You may need to adjust the feature dimensions\n",
        "    adjacency_matrix = np.zeros((num_atoms, num_atoms))\n",
        "\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        adjacency_matrix[i, j] = adjacency_matrix[j, i] = 1  # Adjacency matrix is symmetric\n",
        "\n",
        "    for atom in mol.GetAtoms():\n",
        "        idx = atom.GetIdx()\n",
        "        atom_features[idx, 0] = atom.GetAtomicNum()  # Atom type or atomic number\n",
        "        atom_features[idx, 1] = atom.GetTotalNumHs()  # Number of hydrogen atoms\n",
        "        atom_features[idx, 2] = atom.GetFormalCharge()  # Formal charge\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    atom_features = torch.tensor(atom_features, dtype=torch.float)\n",
        "\n",
        "    # Create edge_index using the adjacency matrix\n",
        "    edge_index = torch.tensor(np.column_stack(np.where(adjacency_matrix)), dtype=torch.long)\n",
        "\n",
        "    # Create PyTorch Geometric data object\n",
        "    data = Data(x=atom_features, edge_index=edge_index.t().contiguous())  # Transpose edge_index\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "XI34cufKkmP0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCN"
      ],
      "metadata": {
        "id": "XpOWk0E4kpYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, input_size, max_len=1000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = nn.Embedding(max_len, input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
        "        positions = positions.expand(x.size(0), -1)  # Expand along batch dimension\n",
        "        return x + self.encoding(positions)"
      ],
      "metadata": {
        "id": "4YgoQX9wkoBE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistanceAttentionEncoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(DistanceAttentionEncoder, self).__init__()\n",
        "\n",
        "        self.embedding = PositionalEncoding(input_size)\n",
        "        self.encoder = nn.Linear(input_size, hidden_size)\n",
        "        self.decoder = nn.Linear(hidden_size, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def pairwise_distances(self, x):\n",
        "        # Calculate pairwise distances using L2 norm\n",
        "        distances = torch.norm(x[:, None, :] - x, dim=-1, p=2)\n",
        "        return distances\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        # Assuming input_sequence has shape (batch_size, sequence_length, input_size)\n",
        "\n",
        "        # Apply positional embeddings\n",
        "        embedded_sequence = self.embedding(input_sequence)\n",
        "\n",
        "        # Encode the embedded sequence\n",
        "        encoded_sequence = self.encoder(embedded_sequence)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        attention_scores = self.decoder(torch.tanh(encoded_sequence))\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "\n",
        "        # Apply attention weights to the encoded sequence\n",
        "        context_vector = torch.sum(encoded_sequence * attention_weights, dim=1)\n",
        "\n",
        "        return context_vector"
      ],
      "metadata": {
        "id": "khkEcrJzktcj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, attention_heads=1):\n",
        "        super().__init__()\n",
        "        self.W_q = nn.Linear(in_channels, out_channels)\n",
        "        self.W_k = nn.Linear(in_channels, out_channels)\n",
        "        self.W_v = nn.Linear(in_channels, out_channels)\n",
        "        self.attention_heads = attention_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply linear transformations to obtain queries, keys, and values\n",
        "        q = self.W_q(x)\n",
        "        k = self.W_k(x)\n",
        "        v = self.W_v(x)\n",
        "\n",
        "        # Reshape queries, keys, and values for multi-head attention\n",
        "        q = q.view(-1, self.attention_heads, q.size(-1))\n",
        "        k = k.view(-1, self.attention_heads, k.size(-1))\n",
        "        v = v.view(-1, self.attention_heads, v.size(-1))\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(q.size(-1), dtype=torch.float32))\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, v).view(x.size(0), -1)\n",
        "\n",
        "        return attention_output"
      ],
      "metadata": {
        "id": "UBEyoidGk25_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1, external_attention_heads=None):\n",
        "        super(GATModel, self).__init__()\n",
        "\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
        "        self.external_attention = MultiHeadAttention(hidden_channels * heads, hidden_channels, attention_heads=external_attention_heads)\n",
        "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)\n",
        "        self.external_attention_heads = external_attention_heads\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # First GAT layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        if self.external_attention_heads is not None:\n",
        "            # External Attention\n",
        "            external_attention_output = self.external_attention(x)\n",
        "\n",
        "            # Concatenate GAT output and external attention output\n",
        "            x = torch.cat([x, external_attention_output], dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Second GAT layer\n",
        "        x = self.conv2(x,edge_index)\n",
        "\n",
        "\n",
        "        self.distance_attention_encoder = DistanceAttentionEncoder(x.size(1), hidden_size=64)\n",
        "\n",
        "        # Apply distance attention encoder\n",
        "        distance_attention_output = self.distance_attention_encoder(x.unsqueeze(0))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "DTKWOBFRkvIo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seperating compunds in the SMILES"
      ],
      "metadata": {
        "id": "WmNO3m8Nk-Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_compounds(smiles_reaction):\n",
        "    # Split the reaction string using '>>' as the separator\n",
        "    compounds = smiles_reaction.split(\">>\")\n",
        "\n",
        "    # Ensure that there are exactly two compounds\n",
        "    if len(compounds) == 2:\n",
        "        reactant = compounds[0].strip()\n",
        "        product = compounds[1].strip()\n",
        "        return reactant, product\n",
        "    else:\n",
        "        raise ValueError(\"Invalid SMILES reaction format. Expected one '>>' separator.\")\n",
        "\n",
        "# Given SMILES reaction\n",
        "smiles_reaction = \"O=C1CCCN1C1CCN(Cc2ccccc2)CC1>>O=C1CCCN1C1CCNCC1\"\n",
        "\n",
        "# Separate compounds\n",
        "reactant, product = separate_compounds(smiles_reaction)\n",
        "\n",
        "# Print the separated compounds\n",
        "print(\"Reactant:\", reactant)\n",
        "print(\"Product:\", product)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkkdPKVjkxgN",
        "outputId": "09663cf5-5b41-45d8-d490-d6560cbcffc9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reactant: O=C1CCCN1C1CCN(Cc2ccccc2)CC1\n",
            "Product: O=C1CCCN1C1CCNCC1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Attention for both\n",
        "Using cross attention to concatenate for both the compounds into a single embedding space"
      ],
      "metadata": {
        "id": "V3hlaPvzlqzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenate_with_cross_attention(emb1, emb2, out_channels, heads):\n",
        "    # Assuming emb1 and emb2 are the output embeddings from two GAT models\n",
        "\n",
        "    # Project the smaller embedding (emb2) to the same dimension as the larger one (emb1)\n",
        "    if emb1.shape[0] > emb2.shape[0]:\n",
        "        linear_projection = nn.Linear(emb2.shape[0], emb1.shape[0])\n",
        "        emb2 = linear_projection(emb2.T).T\n",
        "\n",
        "    elif emb1.shape[0] < emb2.shape[0]:\n",
        "        linear_projection = nn.Linear(emb1.shape[0], emb2.shape[0])\n",
        "        emb1 = linear_projection(emb1.T).T\n",
        "\n",
        "    # Concatenate the embeddings along the feature dimension\n",
        "    concatenated_emb = torch.cat((emb1, emb2), dim=1)\n",
        "\n",
        "\n",
        "    # Apply cross-attention using MultiheadAttention\n",
        "    multihead_attention = nn.MultiheadAttention(embed_dim=2*out_channels, num_heads=heads)\n",
        "    cross_attended_emb, _ = multihead_attention(concatenated_emb, concatenated_emb, concatenated_emb)\n",
        "\n",
        "\n",
        "    return cross_attended_emb"
      ],
      "metadata": {
        "id": "z51KzMZ_lm_0"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cross_attention_output(smiles_string):\n",
        "  reactant, product = separate_compounds(smiles_string)\n",
        "  graph_data_reactant = smiles_to_graph(reactant)\n",
        "  graph_data_product = smiles_to_graph(product)\n",
        "\n",
        "  #for the reactant\n",
        "  in_channels = graph_data_reactant.x.size(1)  # Number of input features\n",
        "  hidden_channels = 64\n",
        "  out_channels = 32\n",
        "  heads = 2  # Number of attention heads\n",
        "  gat_model_reactant = GATModel(in_channels, hidden_channels, out_channels, heads).to(device)\n",
        "  output_reactant = gat_model_reactant(graph_data_reactant)\n",
        "\n",
        "  #for the product\n",
        "  in_channels = graph_data_product.x.size(1)  # Number of input features\n",
        "  hidden_channels = 64\n",
        "  out_channels = 32\n",
        "  heads = 2  # Number of attention heads\n",
        "  gat_model_product = GATModel(in_channels, hidden_channels, out_channels, heads).to(device)\n",
        "  output_product = gat_model_product(graph_data_product)\n",
        "\n",
        "  #applying the cross attention\n",
        "  out=concatenate_with_cross_attention(output_reactant, output_product, out_channels=32, heads=2)\n",
        "\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "dL2cMcK7nrFh"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smiles_string = \"O=C1CCCN1C1CCN(Cc2ccccc2)CC1>>O=C1CCCN1C1CCNCC1\"\n",
        "out=get_cross_attention_output(smiles_string)"
      ],
      "metadata": {
        "id": "rk8qs9F5nMgG"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjI3jxaCvcC3",
        "outputId": "93e24e92-ef63-4a27-9ba5-4f39857a1fb6"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([41, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kye_st0Ovt-Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}