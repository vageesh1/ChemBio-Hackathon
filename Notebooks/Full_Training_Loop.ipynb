{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We now have our ready encoder architecture, now we need to make the decoder which takes input of encoders and then preidcting out the predictions for our reactions which will predict the reagent, solvent and catalyst in this case"
      ],
      "metadata": {
        "id": "AlywzJykjt7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "Torvu19BkYjx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Yrm9rKotjTfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f04c6c4-8eac-4845-a694-6ccf4b7285a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.10/dist-packages (2022.9.5)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (9.4.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.11.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-geometric rdkit-pypi fasttext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import spacy\n",
        "import fasttext\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,dataloader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence,pad_packed_sequence,pad_sequence\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "eBVbQeI2keuS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import RDLogger\n",
        "\n",
        "Chem.MolFromSmiles('c1cncc1')\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "Chem.MolFromSmiles('c1cncc1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "day2MEbb7vH9",
        "outputId": "8146c18c-a81c-4233-f3a1-76290cc1bff6"
      },
      "execution_count": 497,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[08:14:25] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "LODSFVELoLPV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JTvgn8OAWSv",
        "outputId": "4ba6f572-aeb4-446d-ba3c-c5c5daab82c9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dataset"
      ],
      "metadata": {
        "id": "qCqR7162_0ZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Chiros Dataset/choriso_public.tsv',sep='\\t')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "MeLSwHSD_15_",
        "outputId": "23db9280-460e-422a-f1e1-12b6c939e854"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         canonic_rxn  \\\n",
              "0  CCO.O=S1(=O)C=Cc2ccccc21.[H][H].[Pd]>>O=S1(=O)...   \n",
              "1  O=S1(=O)C=Cc2ccccc21.[Na+].[OH-].[Zn]>>O=S1(=O...   \n",
              "2  CCO.O=S1(=O)C=Cc2ccccc21.[Pd]>>O=S1(=O)CCc2ccc...   \n",
              "3  CO.O=C1CCCN1C1CCN(Cc2ccccc2)CC1.O=C[O-].[NH4+]...   \n",
              "4  CC(C)(C)OC(=O)N1CC2CC(CN(Cc3ccccc3)C2)C1.CCO.[...   \n",
              "\n",
              "                                       rxnmapper_aam  \\\n",
              "0  CCO.[O:1]=[S:2]1(=[O:3])[CH:4]=[CH:5][c:6]2[cH...   \n",
              "1  [O:1]=[S:2]1(=[O:3])[CH:4]=[CH:5][c:6]2[cH:7][...   \n",
              "2  CCO.[O:1]=[S:2]1(=[O:3])[CH:4]=[CH:5][c:6]2[cH...   \n",
              "3  CO.[O:1]=[C:2]1[CH2:3][CH2:4][CH2:5][N:6]1[CH:...   \n",
              "4  [CH3:1][C:2]([CH3:3])([CH3:4])[O:5][C:6](=[O:7...   \n",
              "\n",
              "                                             reagent   solvent  \\\n",
              "0                                           hydrogen   ethanol   \n",
              "1                              sodium hydroxide|zinc     empty   \n",
              "2            palladium on activated charcoal|ethanol     empty   \n",
              "3  palladium 10 on activated carbon|ammonium formate  methanol   \n",
              "4                                           hydrogen   ethanol   \n",
              "\n",
              "                          catalyst  yield  \n",
              "0  palladium on activated charcoal  100.0  \n",
              "1                            empty    0.0  \n",
              "2                            empty    0.0  \n",
              "3                            empty  100.0  \n",
              "4  palladium on activated charcoal   51.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-84ba70e3-8305-48b6-a50a-a765b57d2e25\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>canonic_rxn</th>\n",
              "      <th>rxnmapper_aam</th>\n",
              "      <th>reagent</th>\n",
              "      <th>solvent</th>\n",
              "      <th>catalyst</th>\n",
              "      <th>yield</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CCO.O=S1(=O)C=Cc2ccccc21.[H][H].[Pd]&gt;&gt;O=S1(=O)...</td>\n",
              "      <td>CCO.[O:1]=[S:2]1(=[O:3])[CH:4]=[CH:5][c:6]2[cH...</td>\n",
              "      <td>hydrogen</td>\n",
              "      <td>ethanol</td>\n",
              "      <td>palladium on activated charcoal</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>O=S1(=O)C=Cc2ccccc21.[Na+].[OH-].[Zn]&gt;&gt;O=S1(=O...</td>\n",
              "      <td>[O:1]=[S:2]1(=[O:3])[CH:4]=[CH:5][c:6]2[cH:7][...</td>\n",
              "      <td>sodium hydroxide|zinc</td>\n",
              "      <td>empty</td>\n",
              "      <td>empty</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CCO.O=S1(=O)C=Cc2ccccc21.[Pd]&gt;&gt;O=S1(=O)CCc2ccc...</td>\n",
              "      <td>CCO.[O:1]=[S:2]1(=[O:3])[CH:4]=[CH:5][c:6]2[cH...</td>\n",
              "      <td>palladium on activated charcoal|ethanol</td>\n",
              "      <td>empty</td>\n",
              "      <td>empty</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CO.O=C1CCCN1C1CCN(Cc2ccccc2)CC1.O=C[O-].[NH4+]...</td>\n",
              "      <td>CO.[O:1]=[C:2]1[CH2:3][CH2:4][CH2:5][N:6]1[CH:...</td>\n",
              "      <td>palladium 10 on activated carbon|ammonium formate</td>\n",
              "      <td>methanol</td>\n",
              "      <td>empty</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CC(C)(C)OC(=O)N1CC2CC(CN(Cc3ccccc3)C2)C1.CCO.[...</td>\n",
              "      <td>[CH3:1][C:2]([CH3:3])([CH3:4])[O:5][C:6](=[O:7...</td>\n",
              "      <td>hydrogen</td>\n",
              "      <td>ethanol</td>\n",
              "      <td>palladium on activated charcoal</td>\n",
              "      <td>51.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84ba70e3-8305-48b6-a50a-a765b57d2e25')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-84ba70e3-8305-48b6-a50a-a765b57d2e25 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-84ba70e3-8305-48b6-a50a-a765b57d2e25');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e449964a-d67a-40dd-9c43-38b98839b354\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e449964a-d67a-40dd-9c43-38b98839b354')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e449964a-d67a-40dd-9c43-38b98839b354 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cZdLhbVUAmc2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features File"
      ],
      "metadata": {
        "id": "NODS63QhkjDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def featurize_molecule(mol):\n",
        "    # Compute Morgan fingerprints for each atom\n",
        "    atom_features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        idx = atom.GetIdx()\n",
        "        atom_feature = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, atomIndices=[idx])\n",
        "        atom_features.append(np.array(atom_feature))\n",
        "\n",
        "    return np.array(atom_features)"
      ],
      "metadata": {
        "id": "7iET2R-Rkhho"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smiles_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    # Add explicit hydrogens\n",
        "    mol = Chem.AddHs(mol)\n",
        "\n",
        "    # Generate 3D coordinates for visualization\n",
        "    AllChem.EmbedMolecule(mol, randomSeed=42)  # You can choose any seed value\n",
        "\n",
        "    # Get atom features and adjacency matrix\n",
        "    num_atoms = mol.GetNumAtoms()\n",
        "    atom_features = np.zeros((num_atoms, 3))  # You may need to adjust the feature dimensions\n",
        "    adjacency_matrix = np.zeros((num_atoms, num_atoms))\n",
        "\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        adjacency_matrix[i, j] = adjacency_matrix[j, i] = 1  # Adjacency matrix is symmetric\n",
        "\n",
        "    for atom in mol.GetAtoms():\n",
        "        idx = atom.GetIdx()\n",
        "        atom_features[idx, 0] = atom.GetAtomicNum()  # Atom type or atomic number\n",
        "        atom_features[idx, 1] = atom.GetTotalNumHs()  # Number of hydrogen atoms\n",
        "        atom_features[idx, 2] = atom.GetFormalCharge()  # Formal charge\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    atom_features = torch.tensor(atom_features, dtype=torch.float)\n",
        "\n",
        "    # Create edge_index using the adjacency matrix\n",
        "    edge_index = torch.tensor(np.column_stack(np.where(adjacency_matrix)), dtype=torch.long)\n",
        "\n",
        "    # Create PyTorch Geometric data object\n",
        "    data = Data(x=atom_features, edge_index=edge_index.t().contiguous())  # Transpose edge_index\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "XI34cufKkmP0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCN"
      ],
      "metadata": {
        "id": "XpOWk0E4kpYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, input_size, max_len=1000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = nn.Embedding(max_len, input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
        "        positions = positions.expand(x.size(0), -1)  # Expand along batch dimension\n",
        "        return x + self.encoding(positions)"
      ],
      "metadata": {
        "id": "4YgoQX9wkoBE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistanceAttentionEncoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(DistanceAttentionEncoder, self).__init__()\n",
        "\n",
        "        self.embedding = PositionalEncoding(input_size)\n",
        "        self.encoder = nn.Linear(input_size, hidden_size)\n",
        "        self.decoder = nn.Linear(hidden_size, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def pairwise_distances(self, x):\n",
        "        # Calculate pairwise distances using L2 norm\n",
        "        distances = torch.norm(x[:, None, :] - x, dim=-1, p=2)\n",
        "        return distances\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        # Assuming input_sequence has shape (batch_size, sequence_length, input_size)\n",
        "\n",
        "        # Apply positional embeddings\n",
        "        embedded_sequence = self.embedding(input_sequence)\n",
        "\n",
        "        # Encode the embedded sequence\n",
        "        encoded_sequence = self.encoder(embedded_sequence)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        attention_scores = self.decoder(torch.tanh(encoded_sequence))\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "\n",
        "        # Apply attention weights to the encoded sequence\n",
        "        context_vector = torch.sum(encoded_sequence * attention_weights, dim=1)\n",
        "\n",
        "        return context_vector"
      ],
      "metadata": {
        "id": "khkEcrJzktcj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, attention_heads=1):\n",
        "        super().__init__()\n",
        "        self.W_q = nn.Linear(in_channels, out_channels)\n",
        "        self.W_k = nn.Linear(in_channels, out_channels)\n",
        "        self.W_v = nn.Linear(in_channels, out_channels)\n",
        "        self.attention_heads = attention_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply linear transformations to obtain queries, keys, and values\n",
        "        q = self.W_q(x)\n",
        "        k = self.W_k(x)\n",
        "        v = self.W_v(x)\n",
        "\n",
        "        # Reshape queries, keys, and values for multi-head attention\n",
        "        q = q.view(-1, self.attention_heads, q.size(-1))\n",
        "        k = k.view(-1, self.attention_heads, k.size(-1))\n",
        "        v = v.view(-1, self.attention_heads, v.size(-1))\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(q.size(-1), dtype=torch.float32))\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, v).view(x.size(0), -1)\n",
        "\n",
        "        return attention_output"
      ],
      "metadata": {
        "id": "UBEyoidGk25_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1, external_attention_heads=None):\n",
        "        super(GATModel, self).__init__()\n",
        "\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
        "        self.external_attention = MultiHeadAttention(hidden_channels * heads, hidden_channels, attention_heads=external_attention_heads)\n",
        "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)\n",
        "        self.external_attention_heads = external_attention_heads\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # First GAT layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        if self.external_attention_heads is not None:\n",
        "            # External Attention\n",
        "            external_attention_output = self.external_attention(x)\n",
        "\n",
        "            # Concatenate GAT output and external attention output\n",
        "            x = torch.cat([x, external_attention_output], dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Second GAT layer\n",
        "        x = self.conv2(x,edge_index)\n",
        "\n",
        "\n",
        "        self.distance_attention_encoder = DistanceAttentionEncoder(x.size(1), hidden_size=64)\n",
        "\n",
        "        # Apply distance attention encoder\n",
        "        distance_attention_output = self.distance_attention_encoder(x.unsqueeze(0))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "DTKWOBFRkvIo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seperating compunds in the SMILES"
      ],
      "metadata": {
        "id": "WmNO3m8Nk-Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_compounds(smiles_reaction):\n",
        "    # Split the reaction string using '>>' as the separator\n",
        "    compounds = smiles_reaction.split(\">>\")\n",
        "\n",
        "    # Ensure that there are exactly two compounds\n",
        "    if len(compounds) == 2:\n",
        "        reactant = compounds[0].strip()\n",
        "        product = compounds[1].strip()\n",
        "        return reactant, product\n",
        "    else:\n",
        "        raise ValueError(\"Invalid SMILES reaction format. Expected one '>>' separator.\")\n",
        "\n",
        "# Given SMILES reaction\n",
        "smiles_reaction = \"O=C1CCCN1C1CCN(Cc2ccccc2)CC1>>O=C1CCCN1C1CCNCC1\"\n",
        "\n",
        "# Separate compounds\n",
        "reactant, product = separate_compounds(smiles_reaction)\n",
        "\n",
        "# Print the separated compounds\n",
        "print(\"Reactant:\", reactant)\n",
        "print(\"Product:\", product)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkkdPKVjkxgN",
        "outputId": "8a8fd7ce-b41a-434e-f51b-c312ceeec6e1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reactant: O=C1CCCN1C1CCN(Cc2ccccc2)CC1\n",
            "Product: O=C1CCCN1C1CCNCC1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Attention for both\n",
        "Using cross attention to concatenate for both the compounds into a single embedding space"
      ],
      "metadata": {
        "id": "V3hlaPvzlqzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenate_with_cross_attention(emb1, emb2, out_channels, heads):\n",
        "    # Assuming emb1 and emb2 are the output embeddings from two GAT models\n",
        "\n",
        "    # Project the smaller embedding (emb2) to the same dimension as the larger one (emb1)\n",
        "    if emb1.shape[0] > emb2.shape[0]:\n",
        "        linear_projection = nn.Linear(emb2.shape[0], emb1.shape[0])\n",
        "        emb2 = linear_projection(emb2.T).T\n",
        "\n",
        "    elif emb1.shape[0] < emb2.shape[0]:\n",
        "        linear_projection = nn.Linear(emb1.shape[0], emb2.shape[0])\n",
        "        emb1 = linear_projection(emb1.T).T\n",
        "\n",
        "    # Concatenate the embeddings along the feature dimension\n",
        "    concatenated_emb = torch.cat((emb1, emb2), dim=1)\n",
        "\n",
        "\n",
        "    # Apply cross-attention using MultiheadAttention\n",
        "    multihead_attention = nn.MultiheadAttention(embed_dim=2*out_channels, num_heads=heads)\n",
        "    cross_attended_emb, _ = multihead_attention(concatenated_emb, concatenated_emb, concatenated_emb)\n",
        "\n",
        "\n",
        "    return cross_attended_emb"
      ],
      "metadata": {
        "id": "z51KzMZ_lm_0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cross_attention_output(smiles_string,train_mode=True):\n",
        "\n",
        "  reactant, product = separate_compounds(smiles_string)\n",
        "  graph_data_reactant = smiles_to_graph(reactant)\n",
        "  graph_data_product = smiles_to_graph(product)\n",
        "  graph_data_reactant=graph_data_reactant.to(device)\n",
        "  graph_data_product=graph_data_product.to(device)\n",
        "\n",
        "  #for the reactant\n",
        "  in_channels = graph_data_reactant.x.size(1)  # Number of input features\n",
        "  hidden_channels = 64\n",
        "  out_channels = 32\n",
        "  heads = 2  # Number of attention heads\n",
        "  gat_model_reactant = GATModel(in_channels, hidden_channels, out_channels, heads).to(device)\n",
        "  if train_mode:\n",
        "    gat_model_reactant.train()\n",
        "  else:\n",
        "    gat_model_reactant.eval()\n",
        "  output_reactant = gat_model_reactant(graph_data_reactant)\n",
        "\n",
        "  #for the product\n",
        "  in_channels = graph_data_product.x.size(1)  # Number of input features\n",
        "  hidden_channels = 64\n",
        "  out_channels = 32\n",
        "  heads = 2  # Number of attention heads\n",
        "  gat_model_product = GATModel(in_channels, hidden_channels, out_channels, heads).to(device)\n",
        "\n",
        "  if train_mode:\n",
        "    gat_model_reactant.train()\n",
        "  else:\n",
        "    gat_model_reactant.eval()\n",
        "\n",
        "  output_product = gat_model_product(graph_data_product)\n",
        "\n",
        "  #applying the cross attention\n",
        "  out=concatenate_with_cross_attention(output_reactant, output_product, out_channels=32, heads=2)\n",
        "\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "dL2cMcK7nrFh"
      },
      "execution_count": 409,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smiles_string = \"CC(C)(C)OC(=O)N1CC2CC(CN(Cc3ccccc3)C2)C1>>CC(C)(C)OC(=O)N1CC2CNCC(C2)C1\"\n",
        "out=get_cross_attention_output(smiles_string)"
      ],
      "metadata": {
        "id": "rk8qs9F5nMgG"
      },
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjI3jxaCvcC3",
        "outputId": "f45ba87c-ee13-4e2f-fbe6-315dfbf08353"
      },
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([51, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocab Size"
      ],
      "metadata": {
        "id": "BV8EL6MN6W_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder\n",
        "This will decode the whole output for the given encoded input, I am taking a 3 head decoder for this task as, we have to predict three different things in this case"
      ],
      "metadata": {
        "id": "slQutCFZ3i6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.iloc[:,:100]"
      ],
      "metadata": {
        "id": "8vMNFwBQQSIi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocab Size and Padding"
      ],
      "metadata": {
        "id": "WJzQOVO5-KH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequence(sequence, max_length, padding_token='<PAD>'):\n",
        "    return sequence + [padding_token] * (max_length - len(sequence))"
      ],
      "metadata": {
        "id": "mTiNvq7aEVrO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.iloc[:100,:]"
      ],
      "metadata": {
        "id": "brUmdMmURaP_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDLtVusHWJ-m",
        "outputId": "abc1f35f-e0f6-4b74-9351-e76c0e147e47"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 6 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   canonic_rxn    100 non-null    object \n",
            " 1   rxnmapper_aam  100 non-null    object \n",
            " 2   reagent        100 non-null    object \n",
            " 3   solvent        100 non-null    object \n",
            " 4   catalyst       100 non-null    object \n",
            " 5   yield          100 non-null    float64\n",
            "dtypes: float64(1), object(5)\n",
            "memory usage: 4.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_process = ['reagent', 'solvent', 'catalyst']\n",
        "\n",
        "for column in columns_to_process:\n",
        "    # Tokenize and build vocabularies for each column\n",
        "    tokens = [token for item in df[column] for token in word_tokenize(str(item))]\n",
        "    vocab = set(tokens)\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # Pad sequences to a common size\n",
        "    max_seq_length = max(len(token) for token in tokens)\n",
        "\n",
        "    # Pad sequences in the DataFrame\n",
        "    df[column] = df[column].apply(lambda x: pad_sequence(word_tokenize(str(x)), max_seq_length))\n",
        "\n",
        "    # Print results\n",
        "    print(f\"{column.capitalize()} Vocabulary Size: {vocab_size}\")\n",
        "    print(f\"Padded {column.capitalize()}:\\n{df[column]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVdngVQf-2iZ",
        "outputId": "09d3b576-7e24-491a-8cca-30e81da937d2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reagent Vocabulary Size: 149\n",
            "Padded Reagent:\n",
            "0     [hydrogen, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, ...\n",
            "1     [sodium, hydroxide|zinc, <PAD>, <PAD>, <PAD>, ...\n",
            "2     [palladium, on, activated, charcoal|ethanol, <...\n",
            "3     [palladium, 10, on, activated, carbon|ammonium...\n",
            "4     [hydrogen, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, ...\n",
            "                            ...                        \n",
            "95    [tetrachloromethane|bromine, <PAD>, <PAD>, <PA...\n",
            "96    [thionyl, chloride, <PAD>, <PAD>, <PAD>, <PAD>...\n",
            "97    [thionyl, chloride, <PAD>, <PAD>, <PAD>, <PAD>...\n",
            "98    [oxalyl, dichloride|n, ,, n-dimethyl-formamide...\n",
            "99    [thionyl, chloride, <PAD>, <PAD>, <PAD>, <PAD>...\n",
            "Name: reagent, Length: 100, dtype: object\n",
            "\n",
            "Solvent Vocabulary Size: 34\n",
            "Padded Solvent:\n",
            "0     [ethanol, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <...\n",
            "1     [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "2     [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "3     [methanol, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, ...\n",
            "4     [ethanol, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <...\n",
            "                            ...                        \n",
            "95    [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "96    [benzene, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <...\n",
            "97    [tetrahydrofuran, <PAD>, <PAD>, <PAD>, <PAD>, ...\n",
            "98    [dichloromethane, <PAD>, <PAD>, <PAD>, <PAD>, ...\n",
            "99    [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "Name: solvent, Length: 100, dtype: object\n",
            "\n",
            "Catalyst Vocabulary Size: 39\n",
            "Padded Catalyst:\n",
            "0     [palladium, on, activated, charcoal, <PAD>, <P...\n",
            "1     [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "2     [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "3     [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "4     [palladium, on, activated, charcoal, <PAD>, <P...\n",
            "                            ...                        \n",
            "95    [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "96    [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "97    [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "98    [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "99    [empty, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <PA...\n",
            "Name: catalyst, Length: 100, dtype: object\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A full Seq2Seq Network"
      ],
      "metadata": {
        "id": "ILFveO43LHG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size_condition1, vocab_size_condition2, vocab_size_condition3, d_model=512, nhead=8, num_layers=6,enc_dim=64):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "\n",
        "        self.vocab_size_condition1=vocab_size_condition1\n",
        "        self.vocab_size_condition2=vocab_size_condition2\n",
        "        self.vocab_size_condition3=vocab_size_condition3\n",
        "        self.d_model=d_model\n",
        "        self.linear_layer=nn.Linear(vocab_size_condition1, d_model)\n",
        "\n",
        "        self.embedding1 = nn.Embedding(vocab_size_condition1, d_model)\n",
        "        self.embedding2 = nn.Embedding(vocab_size_condition2, d_model)\n",
        "        self.embedding3 = nn.Embedding(vocab_size_condition3, d_model)\n",
        "\n",
        "        self.transformer_layers = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model, nhead),\n",
        "            num_layers\n",
        "        )\n",
        "\n",
        "        # self.linear_proj=nn.Linear()\n",
        "\n",
        "        # Linear layers for each condition\n",
        "        self.fc_condition1 = nn.Linear(d_model, vocab_size_condition1)\n",
        "        self.fc_condition2 = nn.Linear(d_model, vocab_size_condition2)\n",
        "        self.fc_condition3 = nn.Linear(d_model, vocab_size_condition3)\n",
        "\n",
        "        # Softmax activations for each condition\n",
        "        self.softmax_condition1 = nn.Softmax(dim=1)\n",
        "        self.softmax_condition2 = nn.Softmax(dim=1)\n",
        "        self.softmax_condition3 = nn.Softmax(dim=1)\n",
        "\n",
        "        #cross attention\n",
        "        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead)\n",
        "\n",
        "        self.linear_proj=nn.Linear(51,enc_dim)\n",
        "\n",
        "        #final linear layer\n",
        "        self.final_linear_layer = nn.Linear(d_model, 51)\n",
        "\n",
        "    def forward(self, encoded_input,target_sequence):\n",
        "        #a linear to project out the encoded input\n",
        "        self.emb_cond1=self.embedding1(target_sequence)\n",
        "        # self.emb_cond2=self.embedding2(target_sequence[1])\n",
        "        # self.emb_cond3=self.embedding3(target_sequence[2])\n",
        "\n",
        "        # a combined embedding space\n",
        "        # self.comb_emb=self.emb_cond1+self.emb_cond2+self.cond3\n",
        "        #passing it our transformer decoder\n",
        "        memory = torch.rand( 1,51,512)\n",
        "        trans_decoder=self.transformer_layers(self.emb_cond1,memory=memory)\n",
        "        new_size = (trans_decoder.size(0) * trans_decoder.size(1), -1)\n",
        "        trans_decoder_2d = trans_decoder.view(*new_size)\n",
        "\n",
        "        # #projecting to the encoder combined space\n",
        "        proj_trans_decoder=self.linear_proj(trans_decoder_2d.T).T\n",
        "\n",
        "        #the cross attention logic to mix both the encoder and decoder into a single attention space\n",
        "        tensor1 = proj_trans_decoder.unsqueeze(1)\n",
        "        tensor1 = tensor1.permute(1,2,0)\n",
        "        tensor2 = encoded_input.unsqueeze(0)\n",
        "        # Calculate attention weights\n",
        "        attention_weights = F.softmax(torch.bmm(tensor1, tensor2.permute(0, 2, 1)), dim=-1)\n",
        "        cross_attention_result = torch.bmm(attention_weights, tensor2)\n",
        "        cross_attention_result = cross_attention_result.squeeze(1)\n",
        "        new_size = (cross_attention_result.size(0) * cross_attention_result.size(1), -1)\n",
        "        cross_attention_result = cross_attention_result.view(*new_size)\n",
        "\n",
        "        #projecting to the final linear layer\n",
        "        final_output = self.final_linear_layer(cross_attention_result.T)\n",
        "        final_output_summed = final_output.sum(dim=0)#summing across all the 64 values\n",
        "        return final_output_summed\n",
        ""
      ],
      "metadata": {
        "id": "rknul2-9LK-Z"
      },
      "execution_count": 472,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerSeq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size_condition1, vocab_size_condition2, vocab_size_condition3,\n",
        "                  decoder_d_model=512, decoder_nhead=8, decoder_layers=6,enc_dim=64\n",
        "                 ):\n",
        "        super(TransformerSeq2Seq, self).__init__()\n",
        "\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = TransformerDecoder(vocab_size_condition1, vocab_size_condition2, vocab_size_condition3,\n",
        "                                          d_model=decoder_d_model, nhead=decoder_nhead, num_layers=decoder_layers,enc_dim=enc_dim)\n",
        "\n",
        "\n",
        "    def forward(self, input_sequence,targets,train_mode=True):\n",
        "        # input_sequence: The input sequence (reactants or products)\n",
        "\n",
        "        self.hidden_state=get_cross_attention_output((input_sequence),train_mode)\n",
        "\n",
        "        # Decoder forward pass\n",
        "        if train_mode:\n",
        "          self.decoder.train()\n",
        "        else:\n",
        "          self.decoder.eval()\n",
        "        output_probs_condition1= self.decoder(self.hidden_state,targets)\n",
        "\n",
        "        return output_probs_condition1"
      ],
      "metadata": {
        "id": "gFJ51FBmwJH0"
      },
      "execution_count": 473,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import GloVe\n",
        "from torchtext.data import get_tokenizer\n",
        "\n",
        "# Use the GloVe pre-trained word embeddings\n",
        "glove = GloVe(name='6B', dim=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgTXRMKU_RxJ",
        "outputId": "de5504c6-a9c5-4b82-d37e-233c28511ff3"
      },
      "execution_count": 501,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 399999/400000 [01:09<00:00, 5753.15it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the spaCy model\n",
        "def token_to_int(token):\n",
        "    # If the token is '<PAD>', return 0\n",
        "    if token == '<PAD>':\n",
        "        return 0\n",
        "    # Otherwise, use your own mapping logic (here, using ASCII code of the first character)\n",
        "    return ord(token[0])\n",
        "\n",
        "# Function to tokenize and pad a sequence\n",
        "def tokenize_and_pad_sequence(sequence, max_seq_length):\n",
        "    tokenized_sequence = [token_to_int(token) for token in sequence]\n",
        "\n",
        "    return tokenized_sequence"
      ],
      "metadata": {
        "id": "mf5PBKCPaG_N"
      },
      "execution_count": 514,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 100  # Adjust this according to your requirements\n",
        "tokenized_cond1 = tokenize_and_pad_sequence(df['reagent'][1], max_seq_length)\n",
        "tensor_cond1 = torch.tensor(tokenized_cond1).unsqueeze(0)  # Assuming batch size is 1\n",
        "\n",
        "# Example usage for the second condition (solvent)\n",
        "tokenized_cond2 = tokenize_and_pad_sequence(df['solvent'][2], max_seq_length)\n",
        "tensor_cond2 = torch.tensor(tokenized_cond2).unsqueeze(0)  # Assuming batch size is 1\n",
        "\n",
        "# Example usage for the third condition (catalyst)\n",
        "tokenized_cond3 = tokenize_and_pad_sequence(df['catalyst'][10], max_seq_length)\n",
        "tensor_cond3 = torch.tensor(tokenized_cond3).unsqueeze(0)  # Assuming batch size is 1"
      ],
      "metadata": {
        "id": "UZMa0bYOc4GX"
      },
      "execution_count": 515,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making DataLoader"
      ],
      "metadata": {
        "id": "Rj6-vh9G-tSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReactionDataset(Dataset):\n",
        "    def __init__(self, dataframe, max_seq_length):\n",
        "        self.dataframe = dataframe\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        reagent_sequence = tokenize_and_pad_sequence(df['reagent'][idx], max_seq_length)\n",
        "        tensor_reagent = torch.tensor(reagent_sequence).unsqueeze(0)\n",
        "\n",
        "        canonic_rxn = self.dataframe['canonic_rxn'][idx]\n",
        "\n",
        "        return canonic_rxn,tensor_reagent"
      ],
      "metadata": {
        "id": "F4aQrGaDyAHD"
      },
      "execution_count": 476,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reaction_dataset = ReactionDataset(df, max_seq_length=100)\n",
        "reaction_dataloader = DataLoader(reaction_dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCCq2VE9un_l",
        "outputId": "e74cd971-5ad0-4308-87cf-a35ed6083cf0"
      },
      "execution_count": 477,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "0LLIdECADcsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, optimizer, num_epochs=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for inputs, targets in reaction_dataloader:\n",
        "            inputs = inputs[0]\n",
        "            targets = targets.to(device)\n",
        "            targets = targets.squeeze(0)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs, targets)\n",
        "            outputs = outputs.unsqueeze(0)\n",
        "\n",
        "            outputs = outputs.float()\n",
        "            targets = targets.float()\n",
        "\n",
        "            # Use Mean Squared Error (MSE) loss\n",
        "            loss = F.mse_loss(outputs, targets)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        average_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}\")\n",
        "\n",
        "# Assuming your model and dataloader are defined somewhere\n",
        "# Create an instance of the model\n",
        "model = TransformerSeq2Seq(vocab_size_condition1=149, vocab_size_condition2=149, vocab_size_condition3=149)\n",
        "\n",
        "# Assuming you have a DataLoader named reaction_dataloader\n",
        "# Initialize the loss function and optimizer\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "train_model(model, reaction_dataloader, optimizer, num_epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD-laSDtrNSr",
        "outputId": "e573163f-8626-45a0-c83f-fb0624ed631d"
      },
      "execution_count": 499,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 3659.9035\n",
            "Epoch 2/5, Loss: 2846.5539\n",
            "Epoch 3/5, Loss: 2315.2599\n",
            "Epoch 4/5, Loss: 1727.4715\n",
            "Epoch 5/5, Loss: 1594.1152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lw5PWhPnBrIb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EUvTGiXy0Up1"
      },
      "execution_count": 487,
      "outputs": []
    }
  ]
}