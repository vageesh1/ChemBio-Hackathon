{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We now have our ready encoder architecture, now we need to make the decoder which takes input of encoders and then predicting out the predictions for our reactions which will predict the reagent, solvent and catalyst in this case"
      ],
      "metadata": {
        "id": "AlywzJykjt7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "Torvu19BkYjx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrm9rKotjTfd"
      },
      "outputs": [],
      "source": [
        "!pip install torch-geometric rdkit-pypi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "eBVbQeI2keuS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "LODSFVELoLPV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features File"
      ],
      "metadata": {
        "id": "NODS63QhkjDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def featurize_molecule(mol):\n",
        "    # Compute Morgan fingerprints for each atom\n",
        "    atom_features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        idx = atom.GetIdx()\n",
        "        atom_feature = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, atomIndices=[idx])\n",
        "        atom_features.append(np.array(atom_feature))\n",
        "\n",
        "    return np.array(atom_features)"
      ],
      "metadata": {
        "id": "7iET2R-Rkhho"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smiles_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    # Add explicit hydrogens\n",
        "    mol = Chem.AddHs(mol)\n",
        "\n",
        "    # Generate 3D coordinates for visualization\n",
        "    AllChem.EmbedMolecule(mol, randomSeed=42)  # You can choose any seed value\n",
        "\n",
        "    # Get atom features and adjacency matrix\n",
        "    num_atoms = mol.GetNumAtoms()\n",
        "    atom_features = np.zeros((num_atoms, 3))  # You may need to adjust the feature dimensions\n",
        "    adjacency_matrix = np.zeros((num_atoms, num_atoms))\n",
        "\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        adjacency_matrix[i, j] = adjacency_matrix[j, i] = 1  # Adjacency matrix is symmetric\n",
        "\n",
        "    for atom in mol.GetAtoms():\n",
        "        idx = atom.GetIdx()\n",
        "        atom_features[idx, 0] = atom.GetAtomicNum()  # Atom type or atomic number\n",
        "        atom_features[idx, 1] = atom.GetTotalNumHs()  # Number of hydrogen atoms\n",
        "        atom_features[idx, 2] = atom.GetFormalCharge()  # Formal charge\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    atom_features = torch.tensor(atom_features, dtype=torch.float)\n",
        "\n",
        "    # Create edge_index using the adjacency matrix\n",
        "    edge_index = torch.tensor(np.column_stack(np.where(adjacency_matrix)), dtype=torch.long)\n",
        "\n",
        "    # Create PyTorch Geometric data object\n",
        "    data = Data(x=atom_features, edge_index=edge_index.t().contiguous())  # Transpose edge_index\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "XI34cufKkmP0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCN"
      ],
      "metadata": {
        "id": "XpOWk0E4kpYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, input_size, max_len=1000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = nn.Embedding(max_len, input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
        "        positions = positions.expand(x.size(0), -1)  # Expand along batch dimension\n",
        "        return x + self.encoding(positions)"
      ],
      "metadata": {
        "id": "4YgoQX9wkoBE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistanceAttentionEncoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(DistanceAttentionEncoder, self).__init__()\n",
        "\n",
        "        self.embedding = PositionalEncoding(input_size)\n",
        "        self.encoder = nn.Linear(input_size, hidden_size)\n",
        "        self.decoder = nn.Linear(hidden_size, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def pairwise_distances(self, x):\n",
        "        # Calculate pairwise distances using L2 norm\n",
        "        distances = torch.norm(x[:, None, :] - x, dim=-1, p=2)\n",
        "        return distances\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        # Assuming input_sequence has shape (batch_size, sequence_length, input_size)\n",
        "\n",
        "        # Apply positional embeddings\n",
        "        embedded_sequence = self.embedding(input_sequence)\n",
        "\n",
        "        # Encode the embedded sequence\n",
        "        encoded_sequence = self.encoder(embedded_sequence)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        attention_scores = self.decoder(torch.tanh(encoded_sequence))\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "\n",
        "        # Apply attention weights to the encoded sequence\n",
        "        context_vector = torch.sum(encoded_sequence * attention_weights, dim=1)\n",
        "\n",
        "        return context_vector"
      ],
      "metadata": {
        "id": "khkEcrJzktcj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, attention_heads=1):\n",
        "        super().__init__()\n",
        "        self.W_q = nn.Linear(in_channels, out_channels)\n",
        "        self.W_k = nn.Linear(in_channels, out_channels)\n",
        "        self.W_v = nn.Linear(in_channels, out_channels)\n",
        "        self.attention_heads = attention_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply linear transformations to obtain queries, keys, and values\n",
        "        q = self.W_q(x)\n",
        "        k = self.W_k(x)\n",
        "        v = self.W_v(x)\n",
        "\n",
        "        # Reshape queries, keys, and values for multi-head attention\n",
        "        q = q.view(-1, self.attention_heads, q.size(-1))\n",
        "        k = k.view(-1, self.attention_heads, k.size(-1))\n",
        "        v = v.view(-1, self.attention_heads, v.size(-1))\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(q.size(-1), dtype=torch.float32))\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, v).view(x.size(0), -1)\n",
        "\n",
        "        return attention_output"
      ],
      "metadata": {
        "id": "UBEyoidGk25_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1, external_attention_heads=None):\n",
        "        super(GATModel, self).__init__()\n",
        "\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
        "        self.external_attention = MultiHeadAttention(hidden_channels * heads, hidden_channels, attention_heads=external_attention_heads)\n",
        "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)\n",
        "        self.external_attention_heads = external_attention_heads\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # First GAT layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        if self.external_attention_heads is not None:\n",
        "            # External Attention\n",
        "            external_attention_output = self.external_attention(x)\n",
        "\n",
        "            # Concatenate GAT output and external attention output\n",
        "            x = torch.cat([x, external_attention_output], dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Second GAT layer\n",
        "        x = self.conv2(x,edge_index)\n",
        "\n",
        "\n",
        "        self.distance_attention_encoder = DistanceAttentionEncoder(x.size(1), hidden_size=64)\n",
        "\n",
        "        # Apply distance attention encoder\n",
        "        distance_attention_output = self.distance_attention_encoder(x.unsqueeze(0))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "DTKWOBFRkvIo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seperating compunds in the SMILES"
      ],
      "metadata": {
        "id": "WmNO3m8Nk-Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_compounds(smiles_reaction):\n",
        "    # Split the reaction string using '>>' as the separator\n",
        "    compounds = smiles_reaction.split(\">>\")\n",
        "\n",
        "    # Ensure that there are exactly two compounds\n",
        "    if len(compounds) == 2:\n",
        "        reactant = compounds[0].strip()\n",
        "        product = compounds[1].strip()\n",
        "        return reactant, product\n",
        "    else:\n",
        "        raise ValueError(\"Invalid SMILES reaction format. Expected one '>>' separator.\")\n",
        "\n",
        "# Given SMILES reaction\n",
        "smiles_reaction = \"O=C1CCCN1C1CCN(Cc2ccccc2)CC1>>O=C1CCCN1C1CCNCC1\"\n",
        "\n",
        "# Separate compounds\n",
        "reactant, product = separate_compounds(smiles_reaction)\n",
        "\n",
        "# Print the separated compounds\n",
        "print(\"Reactant:\", reactant)\n",
        "print(\"Product:\", product)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkkdPKVjkxgN",
        "outputId": "7ab64920-4648-4d6e-862c-7ebed82e8029"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reactant: O=C1CCCN1C1CCN(Cc2ccccc2)CC1\n",
            "Product: O=C1CCCN1C1CCNCC1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Attention for both\n",
        "Using cross attention to concatenate for both the compounds into a single embedding space"
      ],
      "metadata": {
        "id": "V3hlaPvzlqzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenate_with_cross_attention(emb1, emb2, out_channels, heads):\n",
        "    # Assuming emb1 and emb2 are the output embeddings from two GAT models\n",
        "\n",
        "    # Project the smaller embedding (emb2) to the same dimension as the larger one (emb1)\n",
        "    if emb1.shape[0] > emb2.shape[0]:\n",
        "        linear_projection = nn.Linear(emb2.shape[0], emb1.shape[0])\n",
        "        emb2 = linear_projection(emb2.T).T\n",
        "\n",
        "    elif emb1.shape[0] < emb2.shape[0]:\n",
        "        linear_projection = nn.Linear(emb1.shape[0], emb2.shape[0])\n",
        "        emb1 = linear_projection(emb1.T).T\n",
        "\n",
        "    # Concatenate the embeddings along the feature dimension\n",
        "    concatenated_emb = torch.cat((emb1, emb2), dim=1)\n",
        "\n",
        "\n",
        "    # Apply cross-attention using MultiheadAttention\n",
        "    multihead_attention = nn.MultiheadAttention(embed_dim=2*out_channels, num_heads=heads)\n",
        "    cross_attended_emb, _ = multihead_attention(concatenated_emb, concatenated_emb, concatenated_emb)\n",
        "\n",
        "\n",
        "    return cross_attended_emb"
      ],
      "metadata": {
        "id": "z51KzMZ_lm_0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cross_attention_output(smiles_string):\n",
        "  reactant, product = separate_compounds(smiles_string)\n",
        "  graph_data_reactant = smiles_to_graph(reactant)\n",
        "  graph_data_product = smiles_to_graph(product)\n",
        "\n",
        "  #for the reactant\n",
        "  in_channels = graph_data_reactant.x.size(1)  # Number of input features\n",
        "  hidden_channels = 64\n",
        "  out_channels = 32\n",
        "  heads = 2  # Number of attention heads\n",
        "  gat_model_reactant = GATModel(in_channels, hidden_channels, out_channels, heads).to(device)\n",
        "  output_reactant = gat_model_reactant(graph_data_reactant)\n",
        "\n",
        "  #for the product\n",
        "  in_channels = graph_data_product.x.size(1)  # Number of input features\n",
        "  hidden_channels = 64\n",
        "  out_channels = 32\n",
        "  heads = 2  # Number of attention heads\n",
        "  gat_model_product = GATModel(in_channels, hidden_channels, out_channels, heads).to(device)\n",
        "  output_product = gat_model_product(graph_data_product)\n",
        "\n",
        "  #applying the cross attention\n",
        "  out=concatenate_with_cross_attention(output_reactant, output_product, out_channels=32, heads=2)\n",
        "\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "dL2cMcK7nrFh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smiles_string = \"O=C1CCCN1C1CCN(Cc2ccccc2)CC1>>O=C1CCCN1C1CCNCC1\"\n",
        "out=get_cross_attention_output(smiles_string)"
      ],
      "metadata": {
        "id": "rk8qs9F5nMgG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjI3jxaCvcC3",
        "outputId": "00c8bd23-5ac2-4b70-89c7-4d1ff0226273"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([41, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocab Size"
      ],
      "metadata": {
        "id": "BV8EL6MN6W_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder\n",
        "This will decode the whole output for the given encoded input, I am taking a 3 head decoder for this task as, we have to predict three different things in this case"
      ],
      "metadata": {
        "id": "slQutCFZ3i6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size_condition1, vocab_size_condition2, vocab_size_condition3, d_model=512, nhead=8, num_layers=6):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "\n",
        "        self.vocab_size_condition1=vocab_size_condition1\n",
        "        self.linear_layer=nn.Linear(vocab_size_condition1, d_model)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size_condition1, d_model)\n",
        "\n",
        "        self.transformer_layers = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model, nhead),\n",
        "            num_layers\n",
        "        )\n",
        "\n",
        "        # Linear layers for each condition\n",
        "        self.fc_condition1 = nn.Linear(d_model, vocab_size_condition1)\n",
        "        self.fc_condition2 = nn.Linear(d_model, vocab_size_condition2)\n",
        "        self.fc_condition3 = nn.Linear(d_model, vocab_size_condition3)\n",
        "\n",
        "        # Softmax activations for each condition\n",
        "        self.softmax_condition1 = nn.Softmax(dim=1)\n",
        "        self.softmax_condition2 = nn.Softmax(dim=1)\n",
        "        self.softmax_condition3 = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, encoded_input):\n",
        "        #a linear to project out the encoded input\n",
        "        self.linear=nn.Linear(encoded_input.size(1),self.vocab_size_condition1)\n",
        "        encoded_input=self.linear(encoded_input)\n",
        "        # encoded_input: The common encoded input from the encoder\n",
        "        embedded=self.linear_layer(encoded_input)\n",
        "        # Expand dimensions to add sequence length dimension\n",
        "\n",
        "\n",
        "        # Transformer decoder layers\n",
        "        memory = torch.rand(32, 512)\n",
        "        transformer_out = self.transformer_layers(embedded,memory=memory)\n",
        "\n",
        "        # Fully connected layers and softmax activations for each condition\n",
        "        output_condition1 = self.fc_condition1(transformer_out)\n",
        "        output_condition2 = self.fc_condition2(transformer_out)\n",
        "        output_condition3 = self.fc_condition3(transformer_out)\n",
        "\n",
        "        output_probs_condition1 = self.softmax_condition1(output_condition1)\n",
        "        output_probs_condition2 = self.softmax_condition2(output_condition2)\n",
        "        output_probs_condition3 = self.softmax_condition3(output_condition3)\n",
        "\n",
        "        return output_probs_condition1, output_probs_condition2, output_probs_condition3\n"
      ],
      "metadata": {
        "id": "1-Qnssuz3yRO"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.randn([41, 64])\n",
        "decoder=TransformerDecoder(vocab_size_condition1=1000, vocab_size_condition2=1000, vocab_size_condition3=1000)"
      ],
      "metadata": {
        "id": "JR5CK6n9lJXi"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out=decoder(x)"
      ],
      "metadata": {
        "id": "4tH1UTU7lWhe"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTJIC02HnQla",
        "outputId": "581d059e-bc7a-40e2-ecfa-2db604b9d191"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[0.0006, 0.0015, 0.0004,  ..., 0.0006, 0.0017, 0.0022],\n",
            "        [0.0007, 0.0017, 0.0004,  ..., 0.0007, 0.0023, 0.0019],\n",
            "        [0.0011, 0.0022, 0.0006,  ..., 0.0005, 0.0021, 0.0017],\n",
            "        ...,\n",
            "        [0.0006, 0.0017, 0.0006,  ..., 0.0009, 0.0019, 0.0016],\n",
            "        [0.0009, 0.0014, 0.0004,  ..., 0.0010, 0.0028, 0.0012],\n",
            "        [0.0008, 0.0016, 0.0006,  ..., 0.0010, 0.0016, 0.0018]],\n",
            "       grad_fn=<SoftmaxBackward0>), tensor([[0.0019, 0.0013, 0.0009,  ..., 0.0015, 0.0008, 0.0004],\n",
            "        [0.0016, 0.0012, 0.0010,  ..., 0.0012, 0.0005, 0.0003],\n",
            "        [0.0020, 0.0014, 0.0013,  ..., 0.0023, 0.0007, 0.0004],\n",
            "        ...,\n",
            "        [0.0014, 0.0012, 0.0013,  ..., 0.0022, 0.0004, 0.0005],\n",
            "        [0.0023, 0.0010, 0.0013,  ..., 0.0015, 0.0009, 0.0005],\n",
            "        [0.0021, 0.0008, 0.0010,  ..., 0.0011, 0.0008, 0.0004]],\n",
            "       grad_fn=<SoftmaxBackward0>), tensor([[0.0035, 0.0003, 0.0021,  ..., 0.0006, 0.0014, 0.0010],\n",
            "        [0.0025, 0.0002, 0.0029,  ..., 0.0006, 0.0022, 0.0011],\n",
            "        [0.0026, 0.0004, 0.0036,  ..., 0.0010, 0.0013, 0.0013],\n",
            "        ...,\n",
            "        [0.0020, 0.0004, 0.0044,  ..., 0.0006, 0.0020, 0.0014],\n",
            "        [0.0032, 0.0002, 0.0031,  ..., 0.0005, 0.0017, 0.0011],\n",
            "        [0.0037, 0.0004, 0.0051,  ..., 0.0007, 0.0016, 0.0009]],\n",
            "       grad_fn=<SoftmaxBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seq2Seq"
      ],
      "metadata": {
        "id": "xWWnfVpWhm8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerSeq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 decoder_input_dim, decoder_d_model, decoder_nhead, decoder_layers,\n",
        "                 vocab_size_condition1, vocab_size_condition2, vocab_size_condition3):\n",
        "        super(TransformerSeq2Seq, self).__init__()\n",
        "\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = TransformerDecoder(vocab_size_condition1, vocab_size_condition2, vocab_size_condition3,\n",
        "                                          d_model=decoder_d_model, nhead=decoder_nhead, num_layers=decoder_layers)\n",
        "\n",
        "        # Linear layer to project encoder output to decoder input dimension\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        # input_sequence: The input sequence (reactants or products)\n",
        "        self.hidden_state=get_cross_attention_output(input_sequence)\n",
        "\n",
        "        # Decoder forward pass\n",
        "        output_probs_condition1, output_probs_condition2, output_probs_condition3 = self.decoder(self.hidden_state)\n",
        "\n",
        "        return output_probs_condition1, output_probs_condition2, output_probs_condition3"
      ],
      "metadata": {
        "id": "VPmZSTsHiiPR"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input_dim = 256  # Replace with the desired input dimension for the decoder (should match encoder_hidden_dim)\n",
        "decoder_d_model = 512  # Replace with the desired d_model for the decoder\n",
        "decoder_nhead = 8  # Replace with the desired number of heads for the decoder\n",
        "decoder_layers = 6  # Replace with the desired number of layers for the decoder\n",
        "\n",
        "seq2seq_model = TransformerSeq2Seq(\n",
        "                                   decoder_input_dim, decoder_d_model, decoder_nhead, decoder_layers,\n",
        "                                   vocab_size_condition1=1000, vocab_size_condition2=1000, vocab_size_condition3=1000)\n",
        "\n",
        "\n",
        "\n",
        "# Forward pass\n",
        "output_probs_condition1, output_probs_condition2, output_probs_condition3 = seq2seq_model(input_sequence='O=C1CCCN1C1CCN(Cc2ccccc2)CC1>>O=C1CCCN1C1CCNCC1')\n",
        "\n",
        "# Display the shapes of the output probabilities"
      ],
      "metadata": {
        "id": "dc24M8zMkdbW"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_probs_condition1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd7g2-vFokA9",
        "outputId": "b3012ec8-2205-4405-d8e2-5234b11491e4"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([41, 1000])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "Rj6-vh9G-tSb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "113AWmjU7Z7M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
