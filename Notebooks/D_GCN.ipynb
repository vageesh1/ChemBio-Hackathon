{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In the previous file we had the features for our graph neural network, now time to use D-GCN(Deep Graph Convulation Network) for doing it<br>\n",
        "The features of this includes\n",
        "1. A GCN that takes the graph features as inputs\n",
        "2. A global Attention Encoder to project it into a latent space\n",
        "3. A full model that combines the GCn and the Global Attention Encoder"
      ],
      "metadata": {
        "id": "-e2c715VKRrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "Vf6SI5E6KPUB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiVG1XItKNrU"
      },
      "outputs": [],
      "source": [
        "!pip install torch-geometric rdkit-pypi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "st_qk4msKuks"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE"
      ],
      "metadata": {
        "id": "6tNI2SsSO527"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features File"
      ],
      "metadata": {
        "id": "oZ4ZIqyDKx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def featurize_molecule(mol):\n",
        "    # Compute Morgan fingerprints for each atom\n",
        "    atom_features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        idx = atom.GetIdx()\n",
        "        atom_feature = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, atomIndices=[idx])\n",
        "        atom_features.append(np.array(atom_feature))\n",
        "\n",
        "    return np.array(atom_features)"
      ],
      "metadata": {
        "id": "pl9avRAtNy9a"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smiles_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    # Add explicit hydrogens\n",
        "    mol = Chem.AddHs(mol)\n",
        "\n",
        "    # Generate 3D coordinates for visualization\n",
        "    AllChem.EmbedMolecule(mol, randomSeed=42)  # You can choose any seed value\n",
        "\n",
        "    # Get atom features and adjacency matrix\n",
        "    num_atoms = mol.GetNumAtoms()\n",
        "    atom_features = np.zeros((num_atoms, 3))  # You may need to adjust the feature dimensions\n",
        "    adjacency_matrix = np.zeros((num_atoms, num_atoms))\n",
        "\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        adjacency_matrix[i, j] = adjacency_matrix[j, i] = 1  # Adjacency matrix is symmetric\n",
        "\n",
        "    for atom in mol.GetAtoms():\n",
        "        idx = atom.GetIdx()\n",
        "        atom_features[idx, 0] = atom.GetAtomicNum()  # Atom type or atomic number\n",
        "        atom_features[idx, 1] = atom.GetTotalNumHs()  # Number of hydrogen atoms\n",
        "        atom_features[idx, 2] = atom.GetFormalCharge()  # Formal charge\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    atom_features = torch.tensor(atom_features, dtype=torch.float)\n",
        "\n",
        "    # Create edge_index using the adjacency matrix\n",
        "    edge_index = torch.tensor(np.column_stack(np.where(adjacency_matrix)), dtype=torch.long)\n",
        "\n",
        "    # Create PyTorch Geometric data object\n",
        "    data = Data(x=atom_features, edge_index=edge_index.t().contiguous())  # Transpose edge_index\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "GajUG5yWNith"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCN\n",
        "Graph Convulation Network For processing the input features with the use of attention"
      ],
      "metadata": {
        "id": "GgQnyep4LdEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, attention_heads=1):\n",
        "        super().__init__()\n",
        "        self.W_q = nn.Linear(in_channels, out_channels)\n",
        "        self.W_k = nn.Linear(in_channels, out_channels)\n",
        "        self.W_v = nn.Linear(in_channels, out_channels)\n",
        "        self.attention_heads = attention_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply linear transformations to obtain queries, keys, and values\n",
        "        q = self.W_q(x)\n",
        "        k = self.W_k(x)\n",
        "        v = self.W_v(x)\n",
        "\n",
        "        # Reshape queries, keys, and values for multi-head attention\n",
        "        q = q.view(-1, self.attention_heads, q.size(-1))\n",
        "        k = k.view(-1, self.attention_heads, k.size(-1))\n",
        "        v = v.view(-1, self.attention_heads, v.size(-1))\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(q.size(-1), dtype=torch.float32))\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, v).view(x.size(0), -1)\n",
        "\n",
        "        return attention_output"
      ],
      "metadata": {
        "id": "0ovSm5adL1YE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1, external_attention_heads=None):\n",
        "        super(GATModel, self).__init__()\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
        "        self.external_attention = MultiHeadAttention(hidden_channels * heads, hidden_channels, attention_heads=external_attention_heads)\n",
        "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)\n",
        "        self.external_attention_heads=external_attention_heads\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # First GAT layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        if self.external_attention_heads is not None:\n",
        "\n",
        "          # External Attention\n",
        "          external_attention_output = self.external_attention(x)\n",
        "\n",
        "          # Concatenate GAT output and external attention output\n",
        "          x = torch.cat([x, external_attention_output], dim=-1)\n",
        "\n",
        "        else:\n",
        "          x=x\n",
        "\n",
        "        # Second GAT layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "K5oOZv4PMd1o"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Attention Encoder"
      ],
      "metadata": {
        "id": "S4rWmoJRPgqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Graph Attention Encoder here is making the use of positional embeddings with the help of pairwise distance between the adjacency matrix"
      ],
      "metadata": {
        "id": "yEPg5EJ2TbmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, input_size, max_len=1000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = nn.Embedding(max_len, input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
        "        positions = positions.expand(x.size(0), -1)  # Expand along batch dimension\n",
        "        return x + self.encoding(positions)"
      ],
      "metadata": {
        "id": "b2BqqpxyYToX"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistanceAttentionEncoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(DistanceAttentionEncoder, self).__init__()\n",
        "\n",
        "        self.embedding = PositionalEncoding(input_size)\n",
        "        self.encoder = nn.Linear(input_size, hidden_size)\n",
        "        self.decoder = nn.Linear(hidden_size, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def pairwise_distances(self, x):\n",
        "        # Calculate pairwise distances using L2 norm\n",
        "        distances = torch.norm(x[:, None, :] - x, dim=-1, p=2)\n",
        "        return distances\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        # Assuming input_sequence has shape (batch_size, sequence_length, input_size)\n",
        "\n",
        "        # Apply positional embeddings\n",
        "        embedded_sequence = self.embedding(input_sequence)\n",
        "\n",
        "        # Encode the embedded sequence\n",
        "        encoded_sequence = self.encoder(embedded_sequence)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        attention_scores = self.decoder(torch.tanh(encoded_sequence))\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "\n",
        "        # Apply attention weights to the encoded sequence\n",
        "        context_vector = torch.sum(encoded_sequence * attention_weights, dim=1)\n",
        "\n",
        "        return context_vector"
      ],
      "metadata": {
        "id": "h2mnM9XKO2A6"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making a Full Pipeline\n",
        "Now making a full pipeline that will contain the GCN And the Global Attention Encoder"
      ],
      "metadata": {
        "id": "azzuOuXDSek-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GATModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1, external_attention_heads=None):\n",
        "        super(GATModel, self).__init__()\n",
        "\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
        "        self.external_attention = MultiHeadAttention(hidden_channels * heads, hidden_channels, attention_heads=external_attention_heads)\n",
        "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)\n",
        "        self.external_attention_heads = external_attention_heads\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # First GAT layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        if self.external_attention_heads is not None:\n",
        "            # External Attention\n",
        "            external_attention_output = self.external_attention(x)\n",
        "\n",
        "            # Concatenate GAT output and external attention output\n",
        "            x = torch.cat([x, external_attention_output], dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Second GAT layer\n",
        "        x = self.conv2(x,edge_index)\n",
        "\n",
        "\n",
        "        self.distance_attention_encoder = DistanceAttentionEncoder(x.size(1), hidden_size=64)\n",
        "\n",
        "        # Apply distance attention encoder\n",
        "        distance_attention_output = self.distance_attention_encoder(x.unsqueeze(0))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "PKBXXVxaSBhH"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example Usage"
      ],
      "metadata": {
        "id": "cja_U3tyYNGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "smiles_string = \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\"\n",
        "graph_data = smiles_to_graph(smiles_string)\n",
        "print('Graph Data Shape:', graph_data.x.size())\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the GAT model\n",
        "in_channels = graph_data.x.size(1)  # Number of input features\n",
        "hidden_channels = 64\n",
        "out_channels = 32\n",
        "heads = 2  # Number of attention heads\n",
        "gat_model = GATModel(in_channels, hidden_channels, out_channels, heads).to(device)\n",
        "\n",
        "# Forward pass\n",
        "output = gat_model(graph_data)\n",
        "\n",
        "# Print the output shape\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZzG9NxzV3ck",
        "outputId": "6b4afbf7-65a3-45de-92c9-9eb5ea299ec6"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph Data Shape: torch.Size([24, 3])\n",
            "Output shape: torch.Size([24, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kI1wSUz8V7IO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}